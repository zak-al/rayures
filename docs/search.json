[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rayures",
    "section": "",
    "text": "Multivariate calculus for machine learning: from theory to building an automatic differentiation system\n\n\n\nMaths\n\n\nMachine learning\n\n\n\n\n\n\n\n\n\n\n\nWhat are heaps and how to implement them in C++\n\n\n\nComputer Science\n\n\nData Structures\n\n\n\n\n\n\n\n\n\n\n\nAn introduction to maximum likelihood estimation and applications to machine learning\n\n\n\nStatistics\n\n\nMachine learning\n\n\n\n\n\n\n\n\n\n\n\nA comprehensive introduction to decision trees\n\n\n\nMachine learning\n\n\n\n\n\n\n\n\n\n\n\nAn introduction to Linear Programming\n\n\n\nMathematics\n\n\nOptimisation\n\n\n\n\n\n\n\n\n\n\n\nRed socks, continued fractions, black socks and Pell’s equations\n\n\n\nMathematics\n\n\nProbability\n\n\n\n\n\n\n\n\n\n\n\nAn optimisation approach to Principal Component Analysis\n\n\n\nMachine learning\n\n\nMathematics\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/fourier.html",
    "href": "posts/fourier.html",
    "title": "Discrete Fourier transform: theory, applications and computation",
    "section": "",
    "text": "1 Introduction\nThe discrete Fourier transform of a finite sequence\n\n\n2 What is the discrete Fourier transform?\nConsider a p-dimensional tensor of complex numbers, (z_k)_{0 \\le k_0 \\le n_0 - 1, \\mathellipsis, 0 \\le k_{p-1} \\le n_{p-1} - 1} for some positive integers n_1, \\mathellipsis, n_p. We define the Fourier transform of (z_k) as the sequence (\\mathcal F(\\boldsymbol z)_k)_{k = 0}^{n-1} such that: \\mathcal F(\\boldsymbol z)_k = \\sum_{(0, \\mathellipsis, 0) \\le j \\le (n_1 - 1, \\mathellipsis, n_p - 1)} \\exp\\left(-2i\\pi \\left\\langle j, \\frac kn\\right\\rangle\\right) z_j, where:\n\n\\langle \\cdot, \\cdot \\rangle denotes the standard scalar product, i.e. \\langle x, y \\rangle = \\sum x_iy_i ;\nthe sum is taken over all the indices of (z_k), i.e. over all sequences (j_i)_{i = 0}^{p-1} of p integers such that 0 \\le j_i \\le n_i ;\n\\frac kn is the sequence of p values defined by (\\frac kn)_i = \\frac{k_i}{n_i}.\n\n\n\n3 Computing the discrete Fourier transform: the Cooley-Tukey FFT algorithm\n\n\n4 The convolution theorem and how to compute convolution and correlation efficiently"
  },
  {
    "objectID": "posts/max_likelihood.html",
    "href": "posts/max_likelihood.html",
    "title": "An introduction to maximum likelihood estimation and applications to machine learning",
    "section": "",
    "text": "The maximum likelihood paradigm describes a rule to find a distribution that best fits a set of examples among a parametrised set of distributions. This article defines maximum likelihood estimators and gives two examples of learning models motivated by this paradigm."
  },
  {
    "objectID": "posts/max_likelihood.html#motivation",
    "href": "posts/max_likelihood.html#motivation",
    "title": "An introduction to maximum likelihood estimation and applications to machine learning",
    "section": "1.1 Motivation",
    "text": "1.1 Motivation\nSuppose you have a parametrised set of probability distributions, characterised by density functions with respect to the counting or Lebesgue measure: \\{f_\\vartheta : X \\to \\R_+\\}_{\\vartheta \\in \\Theta}.\nSince these measures are closed under translation, we can view the value of their density f_\\vartheta(x) at some point x \\in \\R as an immediate probability (note that this name can be misleading as it is not a probability in the case of Lebesgue-continuous distributions). If we work with the counting measure, this is clear since the value of the density at some point is the actual probability of the point. When working with the Lebesgue measure, this get slightly more subtle. If the density function is continuous, we can formally justify this intuition by noticing that f_\\vartheta(x) is the limit of the average probability around x: \\lim_{\\delta \\to 0} \\frac{1}{\\delta} \\int_{\\left[x - \\frac\\delta2, x + \\frac\\delta2\\right]} f_\\vartheta\\ \\text d\\mu, which is the fundamental theorem of analysis. This result can be generalised using the Lebesgue differentiation theorem (Wikipedia)"
  },
  {
    "objectID": "posts/max_likelihood.html#general-definition",
    "href": "posts/max_likelihood.html#general-definition",
    "title": "An introduction to maximum likelihood estimation and applications to machine learning",
    "section": "1.2 General definition",
    "text": "1.2 General definition\nFrom this intuition, we can define the likelihood of a sequence of independent and identically-distributed observations \\mathbf{x} = \\{x_1, \\mathellipsis, x_n\\} as L(\\vartheta) = f_\\vartheta^n(\\mathbf{x}), where f_\\vartheta^n is the joint probability distribution over sequences of n independent f_\\vartheta-distributed examples. We then have: L(\\vartheta) = \\prod_{j = 1}^n f_\\vartheta(x_j).\nFrom a computational perspective, we often prefer to deal with the \\log-likelihood of a sequence of examples rather than their likelihood:\n\\log L(\\vartheta) = \\sum_{j = 1}^n \\log\\left(f_\\vartheta(x_j)\\right).\nWe then define the maximum-likelihood estimator of \\vartheta as:\n\\hat\\vartheta_{\\text{ml}} = \\argmax_{\\vartheta \\in \\Theta} L(\\vartheta),\nwhere the existence and uniqueness of the argmax is assumed.\nNote that since \\log is strictly increasing, \\argmax_\\vartheta L(\\vartheta) = \\argmax_\\vartheta (\\log(L(\\vartheta)))."
  },
  {
    "objectID": "posts/max_likelihood.html#approximating-distribution-of-inputoutput-pairs-with-conditional-maximumum-likelihood",
    "href": "posts/max_likelihood.html#approximating-distribution-of-inputoutput-pairs-with-conditional-maximumum-likelihood",
    "title": "An introduction to maximum likelihood estimation and applications to machine learning",
    "section": "1.3 Approximating distribution of input/output pairs with conditional maximumum likelihood",
    "text": "1.3 Approximating distribution of input/output pairs with conditional maximumum likelihood\nSimilarly, we can use the maximum likelihood paradigm to find a distribution that best describes a set of observations of the form \\{(x_1, y_1), \\mathellipsis, (x_n, y_n)\\}, where y_j is viewed as an output associated with input x_j. For every x_j, we consider the density functions f_\\vartheta(\\cdot | x_j) across all \\vartheta \\in \\Theta. We then define the likelihood of the set of observations for parameter \\vartheta as the density function of the random vector sequence (Y_1, \\mathellipsis, Y_n), where Y_j \\sim f_\\theta(\\cdot | x_j), evaluated at (y_1, \\mathellipsis, y_n) i.e. L(\\vartheta) = \\prod_{j = 1}^n f_\\vartheta(y_j | x_j)."
  },
  {
    "objectID": "posts/vector_calculus.html",
    "href": "posts/vector_calculus.html",
    "title": "Multivariate calculus for machine learning: from theory to building an automatic differentiation system",
    "section": "",
    "text": "Many modern machine learning models are trained using gradient-based approaches. This means that at every iteration of the training process, parameters are adjusted based on the value of the differential of the cost incurred by a set of training examples so as to make the differential closer to zero. In order to efficiently keep track of the derivative of the cost with respect to every parameter, machine learning libraries rely on automatic differentiation systems. The goal of this article is to lay the theoretical foundations of multivariate differential calculus from the perspective of machine learning and to show how to implement an automatic differentiation system from scratch, in C++."
  },
  {
    "objectID": "posts/vector_calculus.html#definition",
    "href": "posts/vector_calculus.html#definition",
    "title": "Multivariate calculus for machine learning: from theory to building an automatic differentiation system",
    "section": "2.1 Definition",
    "text": "2.1 Definition\nLet U and V be complex vector spaces, with U being finitely generated with dimension n \\ge 1. Given a fixed canonical basis (e_1, \\mathellipsis, e_n) of U, we say that a function f : U \\to V admits an i-th partial derivative at point x if the function\n\\begin{align*}\n\\lambda \\mapsto \\frac{f(x + \\lambda e_i) - f(x)}{\\lambda}\n\\end{align*}\nhad a limit at 0.\nThe i-th partial derivative at x_0 is then the limit of the above function. \\partial_i f denotes the function that maps x \\in U to the i- th partial derivative of f at x."
  },
  {
    "objectID": "posts/vector_calculus.html#partial-derivatives-and-differentials",
    "href": "posts/vector_calculus.html#partial-derivatives-and-differentials",
    "title": "Multivariate calculus for machine learning: from theory to building an automatic differentiation system",
    "section": "2.2 Partial derivatives and differentials",
    "text": "2.2 Partial derivatives and differentials\nIt can be shown that if a function is differentiable at point x_0 then it admits an i-th partial derivative at x_0, for every i. The converse is not true, however it is sufficient to admit continuous partial derivatives along every component at some point x_0 to be differentiable at x_0. Moreover, the i-th partial derivative of f at x_0 is given by D_{x_0}f(e_i). This means that if V is finitely generated as well then the matrix of D_{x_0}f is (\\partial_j f_i(x_0))_{1 \\le i \\le \\dim(V), 1 \\le j \\le n}. This matrix is known as the Jacobian matrix (of f, at point x_0), that we’ll denote [D_{x_0}f] (or [D_{x_0}f]_{(e_i)} if the choice of the canonical basis is not trivial), or J_{x_0}(f). It can be computed from the functional representation of the differential by plugging in each vector of a basis of the domain.\nIt follows from the calculation rules we saw in the first section that the Jacobian matrix of a linear combination of two functions is the linear combination of their Jacobians, and that the Jacobian of the composition g \\circ f at point x_0 is: [D_{x_0}(g\\circ f)] = [D_{f(x_0)}g] [D_{x_0}f], where juxtaposition denotes matrix multiplication. This formula allows us to express the partial derivatives of"
  },
  {
    "objectID": "posts/heaps.html",
    "href": "posts/heaps.html",
    "title": "What are heaps and how to implement them in C++",
    "section": "",
    "text": "This article introduces heaps and shows how to implement them and the basic operations they support in C++."
  },
  {
    "objectID": "posts/heaps.html#accessing",
    "href": "posts/heaps.html#accessing",
    "title": "What are heaps and how to implement them in C++",
    "section": "3.1 Accessing",
    "text": "3.1 Accessing\nIt follows from the heap property that the smallest element of the set of nodes is always the root. This allows us to perform min-peek in constant time, simply by accessing an element of the array by its index.\nstd::optional&lt;T&gt; peek() {\n    if (n == 0) {\n        return {};\n    }\n\n    return array[1];\n}"
  },
  {
    "objectID": "posts/heaps.html#removing",
    "href": "posts/heaps.html#removing",
    "title": "What are heaps and how to implement them in C++",
    "section": "3.2 Removing",
    "text": "3.2 Removing\nHere is a rough description of the algorithm we use to delete the minimum:\n\nreplace the root of the tree with the last element in the tree;\nwhile the last element has at least one child and is greater than one of its children, swap it with its smallest child.\n\nThe image below shows the configuration of a heap throughout the execution of this algorithm.\n\n\n\nIllustration of the pop procedure.\n\n\nThis algorithm runs in linear time with respect to the height of the heap, i.e. in logarithmic time with respect to the number of elements in the heap.\nTo implement it in C++, we start by defining the function rearrangeDown that takes the index of a node and swaps it with its smallest child if appropriate (i.e. if it is not already smaller than its children). It returns the new position of the input node.\nsize_t rearrangeDown(size_t i) {\n    size_t m = i;\n    std::optional&lt;T&gt; l = getLeftChild(i);\n    std::optional&lt;T&gt; r = getRightChild(i);\n    if (l.has_value() && l.value() &lt; array[i]) {\n        m = 2 * i;\n    }\n    if (r.has_value() && r.value() &lt; array[m]) {\n        m = 2 * i + 1;\n    }\n\n    if (m != i) {\n        std::swap(array[m], array[i]);\n        return m;\n    }\n\n    return i;\n}\nm denotes the smallest value between the node at position i, its left child (if it has one) and its right child (if it has one). If i has two children X and Y and the trees rooted at X and at Y both satisfy the heap property, then it can be proved that the tree rooted at i after executing i = rearrangeDown(i) until rearrangeDown(i) == i satisfies the heap property as well.\nWe now implement pop as follows:\nstd::optional&lt;T&gt; pop() {\n    if (n == 0) {\n        return {};\n    }\n\n    T m = array[1];\n\n    array[1] = array[n];\n    size_t i {1};\n    while (true) {\n        size_t newIdx = rearrangeDown(i);\n        if (newIdx == i) break;\n        i = newIdx;\n    }\n\n    --n;\n\n    return m;\n}"
  },
  {
    "objectID": "posts/heaps.html#algorithm",
    "href": "posts/heaps.html#algorithm",
    "title": "What are heaps and how to implement them in C++",
    "section": "5.1 Algorithm",
    "text": "5.1 Algorithm\nLet’s start with an example. Consider the following nearly-complete binary tree.\n\n\n\nA nearly-complete tree to heapify.\n\n\nThe leaves are clearly roots of heaps. If the list we want to heapify has length n then a node k is a leaf if and only if 2k &gt; n, i.e. if and only if k \\ge n/2 + 1 (where \\cdot / \\cdot again denotes euclidean division). We want to make every other node the root of a heap. Let’s start with 3. 3 is less than 6 so (3, 6) satisfies the heap property. Moving to 4. 4 is greater than one of its children (both!), so we swap it with its smallest child, 1. 4 is now a leaf, so it is indeed the root of a heap. Since the tree rooted at 2 and the tree rooted at 1 were both heaps, the tree rooted at 1 in the updated tree is a heap as well. The last node we need to consider is 5. Since 5 &gt; 3 &gt; 1, we swap 5 with 1. 5 is still greater than its children (2 and 4); so we swap 5 with 2. 5 is now a leaf, and the whole tree has become a heap.\nWe implement it as follows:\ntemplate&lt;typename T&gt;\nHeap&lt;T&gt; heapify(std::vector&lt;T&gt; data) {\n    Heap&lt;T&gt; heap;\n    heap.n = data.size();\n    heap.array = {0};\n    for (T x: data) {\n        heap.array.push_back(x);\n    }\n\n    for (size_t i = heap.n / 2; i &gt; 0; --i) {\n        size_t k;\n        size_t j {i};\n        while ((k = heap.rearrangeDown(j)) != j) {\n            j = k;\n        }\n    }\n\n    return heap;\n}\nLines 11 to 15 do exactly the same as lines 9 to 14 in the insert procedure, i.e. it rearrange down the node which is originally at position i until rearrangeDown does not change its position."
  },
  {
    "objectID": "posts/heaps.html#analysis",
    "href": "posts/heaps.html#analysis",
    "title": "What are heaps and how to implement them in C++",
    "section": "5.2 Analysis",
    "text": "5.2 Analysis\nThe sub-procedure in lines 11 to 16 runs in O(\\text{height}(i)) time, for every i between 1 and n/2. Let H be the height of the heap, i.e. H = \\lfloor \\log_2(n) \\rfloor. For every height h between 1 and H (we’re not considering nodes with height 0), there are 2^{H-h} nodes with height h and running the procedure on each of them incurs a cost bounded by h. Therefore, each height h incurs a cost bounded by h2^{H-h} and the overall complexity is given by:\n\\sum_{k = 1}^H h 2^{H-h} = n \\sum_{k = 0}^H h\\frac1{2^h}.\nSince \\sum h\\frac{1}{2^h} converges (which can be shown using an argument involving differentiation of power series), the complexity is linear in the size n of the tree."
  },
  {
    "objectID": "posts/decision_trees.html",
    "href": "posts/decision_trees.html",
    "title": "A comprehensive introduction to decision trees",
    "section": "",
    "text": "Decision trees are machine learning models used for both classification and regression tasks on data that can be represented by one-dimensional vectors of features. One of their main advantages is that they are easily interpretable. Instead of approximating an optimal prediction function by tweaking numeric parameters that have no conceptual meaning, decision trees try to figure out how every feature contributes to making an accurate prediction. Although raw decision trees are no longer used in many industrial applications, more complex and useful models, like tree ensembles and gradient-boosted decision trees, derive from them.\nThis articles introduces decision trees and describes an algorithm used to make predictions and for training. It focuses on classification tasks and training based on information gain. We’ll formally motivate the training algorithms using information theory and implement a set of utils for prediction and inference in Python."
  },
  {
    "objectID": "posts/decision_trees.html#what-needs-to-be-trained",
    "href": "posts/decision_trees.html#what-needs-to-be-trained",
    "title": "A comprehensive introduction to decision trees",
    "section": "2.1 What needs to be trained",
    "text": "2.1 What needs to be trained\nTraining a decision tree means choosing decisions to apply to incoming data at every node. The generic idea is that we want to select relevant decisions (i.e. ones that allow to efficiently discriminate between classes) without selecting too many decisions (which would lead to overfitting). For example, if we’re trying to train a decision tree to tell whether a sentence is in French or in English, the occurrence counts of trigrams eau and ity are more relevant than the number of occurrences of tio and pbt.\n:::{callout-note title=“Overfitting”} Decision are particularly prone to overfitting because they impose little limit on the structure of the function that maps inputs to predictions. For example, we might just end up having a decision tree deep enough for every data point to end in its own leaf, that completely characterise it. This is why it is not only important to select relevant decisions (all decisions are relevant to some extent), we also need to choose a suitable stopping criterion that prevents the tree from getting too deep. :::\nIn this section, we’ll introduce concepts from information theory that are used to quantify the usefulness of a decision to maximise the confidence with which we’ll be able to assert that our prediction is accurate. We’ll motivate, define formally and implement in Python each of these concepts."
  },
  {
    "objectID": "posts/decision_trees.html#a-brief-introduction-to-information-theory",
    "href": "posts/decision_trees.html#a-brief-introduction-to-information-theory",
    "title": "A comprehensive introduction to decision trees",
    "section": "2.2 A brief introduction to information theory",
    "text": "2.2 A brief introduction to information theory\n\n2.2.1 Information\nWhen training a decision tree on a dataset D = \\{d_1, \\mathellipsis, d_N\\}, we want to choose a decision that holds as much information as possible on the class of the data points.\nAn information function is a continuous function I : (0, 1] \\to [0, +\\infty) that satisfies the following conditions:\n\nfor all x, y, I(xy) = I(x) + I(y): if two events are independent, the information that their intersection provides is the sum of the information that each provides;\nI is strictly decreasing: if an event is less likely to happen than an another, knowing that it happened brings more information.\n\nWe can prove that information functions are exactly the functions \\log_b, where b \\in (0, 1). Equivelently, they are the functions -\\log_b where b &gt; 1. Here is an outline of a proof, given some information function I:\n\nI is a bijection from (0, 1] onto [0, +\\infty): injectivity comes from the fact that is is strictly monotonic and unboundedness comes from the fact that I(1/2^n) = nI(1/2).\nI has an inverse E : [0, +\\infty) \\to (0, 1] that satisfies E(x + y) = E(x)E(y) for all x, y.\nE is continous and agrees with the function r \\mapsto E(1)^r on [0, \\infty) \\cap \\mathbb{Q}, therefore it is the exponential function with base E(1) \\in (0, 1).\nI is therefore the inverse of \\exp_{E(1)}, so I = \\log_{E(1)}.\n\nWe then extend I to [0, 1] by defining I(0) = \\lim_{0} I = +\\infty (which preserves the two axioms).\n\n\n2.2.2 Entropy\nLet \\mathbb P : \\mathcal P(X) \\to [0, 1] be a probability measure with a finite sample space X and let p: X \\to [0, 1] be its mass function. We define the entropy of \\mathbb P, denoted H(\\mathbb P), as the expected value of I \\circ p with respect to \\mathbb P: H(\\mathbb P) = \\sum_{x \\in X} I(p(x)) p(x). (H is a capital \\eta, and should be pronounced eta and not eitsch…)\nFor convenience, we may use H(Y) to denote the entropy of the distribution of a random variable Y.\n\n\n\n\n\n\nNote\n\n\n\nFor simplicity, we pretend that there is a unique information function as the choice of the base does not matter for our purposes. For visualisation purposes, it is convenient to chose base \\frac{1}{|X|} (i.e. I = -\\log_{|X|}) so that H takes values between 0 and 1 (more on that just below).\n\n\nHere is our implementation:\nimport numpy as np\n\ndef calculate_entropy(distribution: np.ndarray) -&gt; float:\n    \"\"\"\n    :param distribution: one-dimensional numpy array describing a probability distribution (i.e. distribution[i] is\n    the probability of the i-th outcome).\n    :return: the entropy of the distribution.\n    \"\"\"\n    indices = distribution != 0\n    return -distribution[indices] @ np.log2(distribution[indices])\nindices contains the indices of entries with non-zero probability. Restricting distribution to indices allows not to pass 0 to log2.\nThe entropy of a probability measure helps us quantify the amount of information carried by the knowledge of the outcome of a random experiment. A high entropy means that knowing the actual outcome gives a lot of information. In other words, it means that knowing the distribution wasn’t sufficient to predict what the outcome was going to be.\nWe can use Jensen’s inequality to show that the uniform distribution maximises the entropy over all distributions over any fixed finite sample space X. On the contrary, the entropy of a distribution that takes value 1 for one outcome and 0 for every other is 0.\nAn important example to remember is the one of a Bernoulli distibution, shown below. \nThe graph shows the entropy of a Bernoulli distribution as a function of its parameter p \\in [0, 1]. If p is close to 0 or 1 then the entropy is low, as almost all the information is carried by the theoretical distribution rather than the knowledge of the actual outcome. If p is close to 1/2, it is difficult to predict what the outcome will be by just looking at the distribution and most of the information is carried by the knowledge of the outcome of the experiment.\n\n\n2.2.3 Conditional entropy and information gain\nLet X be a random variable onto a finite domain (which we’ll assume without loss of generality to be \\{1, \\mathellipsis, n\\} for some positive integer n) and let A be an event. We define the conditional entropy of X given A as the entropy of the conditional distribution of the value of X given A, i.e.: H(X | A) = \\sum_{k = 1}^n \\mathbb P(X = k | A) I(\\mathbb P(X = k | A)).\nA high conditional entropy means that despite knowing that A occurred, we couldn’t predict what value X was going to take. A low conditional entropy means that the assumption that A occurred was enough to make the distribution of X “almost deterministic”.\nWe can then define the conditional entropy of X given another random variables Y taking values in another finite set, let’s say \\{1, \\mathellipsis, p\\} as: H(X | Y) = \\mathbb E_{y \\sim \\mathbb P_Y}[H(X | Y = y)] = \\sum_{y = 1}^p \\mathbb P(Y = y) H(X | Y = y).\nIf H(X | Y) is high, knowing what value was taken by Y won’t help us determine the value that X will take. If H(X | Y) is low, it is enough to know the value taken by Y to be able to predict what value X will take with reasonable confidence.\nTo calculate the conditional distribution of some variable X given another variable Y, we need to know their joint distribution (i.e. the distribution of the random variable (X, Y)). In practice, the information we initially have on the joint distribution is a dataset, i.e. a list of entries, each having two numerical values indicating the value of X and the value of Y. Here is how we’ll compute it:\nCONDITIONING_VARIABLE = 0\nCONDITIONED_VARIABLE = 1\n\ndef calculate_empirical_distribution(dataset: np.ndarray, return_domain: bool = False) -&gt; np.ndarray | Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Calculates the empirical distribution defined by a dataset, i.e. such that the probability of x is the number of\n    occurrences of x in the dataset divided by the total number of elements in the dataset.\n    :param dataset: numpy array.\n    :param return_domain: default to False. If True, returns a tuple (domain, distribution) where domain is\n    the same shape as distribution and distribution[i] is the probability of domain[i].\n    :return: if return_conditioning_domain is set to False: a permutation of the empirical distribution;\n    otherwise: a tuple (domain, distribution) where domain is the same shape as distribution and distribution[i] is\n    the probability of domain[i].\n    \"\"\"\n    n, = dataset.shape\n    domain, unnormalised_distribution = np.unique(dataset, axis=0, return_counts=True)\n    if return_domain:\n        return domain, unnormalised_distribution / n\n\n    return unnormalised_distribution / n\n\ndef calculate_conditional_entropy_from_dataset(dataset: np.ndarray) -&gt; float:\n    \"\"\"\n    Calculate the entropy of the conditioned variable given the value of the conditioning variable.\n    :param dataset: two-dimensional numpy array. Each row describes a data point. The first column denotes the\n    conditioning variable and the second column denotes the conditioned variable.\n    assert dataset.ndim == 2\n    assert dataset.shape[1] == 2\n    :return: conditional entropy\n    \"\"\"\n    assert dataset.ndim == 2\n    assert dataset.shape[1] == 2\n\n    n, _ = dataset.shape  # number of entries in the dataset\n    conditioning_domain, conditioning_distribution = calculate_empirical_distribution(dataset[:, CONDITIONING_VARIABLE],\n                                                                                      return_domain=True)\n    conditioned_variable_domain_cardinality, = np.unique(dataset[:, CONDITIONED_VARIABLE]).shape\n\n    # conditional_entropies[i] is the entropy of the conditioned variable for rows where the conditioning variable\n    # takes value conditioning_domain[i].\n\n    conditioning_domain_cardinality, = conditioning_domain.shape\n\n    conditional_entropies = np.zeros(conditioning_domain_cardinality, dtype=float)\n    for i in range(conditioning_domain_cardinality):\n        conditioned_dataset = dataset[dataset[:, CONDITIONING_VARIABLE] == conditioning_domain[i]]\n        conditional_entropies[i] = calculate_entropy_from_dataset(conditioned_dataset[:, CONDITIONED_VARIABLE])\n    return conditioning_distribution @ conditional_entropies\nWe first created a utility function calculate_empirical_distribution that computes the distribution of values present in a dataset.\n\n\n\n\n\n\nA different formula\n\n\n\nIf A \\in \\R^{1, p} is the row matrix whose y-th entry is \\mathbb P(Y = y) and B \\in \\R^{p, n} is the matrix whose (y, x)-th entry is \\mathbb P(X = x | Y = y) then the formula for the conditional entropy of X given Y is: H(X | Y) = \\sum_{x = 1}^n (A \\times (B \\odot I(B)))_x, where \\odot denotes the Hadamard product, i.e. cell-wise multiplication.\n\n\nFinally, we use the information gain of X relative to Y, \\text{IG}(X, Y) = H(X) - H(X | Y) to quantify the information brought by the knowledge of the value taken by Y to predict the value that X will take.\ndef calculate_information_gain(dataset: np.ndarray) -&gt; float:\n    \"\"\"\n    Calculate the information gain of the conditioning variable on the conditioned variable.\n    :param dataset: two-dimensional numpy array. Each row describes a data point. The first column denotes the\n    conditioning variable and the second column denotes the conditioned variable.\n    :return: information gain\n    \"\"\"\n    prior_entropy = calculate_entropy_from_dataset(dataset[:, CONDITIONED_VARIABLE])\n    posterior_entropy = calculate_conditional_entropy_from_dataset(dataset)\n    return prior_entropy - posterior_entropy\n\n\n\n\n\n\nNote\n\n\n\nMaximum purity is reached where the information gain is the highest, or equivalently where conditional entropy is the lowest. Therefore, you might be wondering why we should bother compute the information gain when we could simply try to minimise the entropy. The reason is that the information gain helps us decide when we should stop training.\nFor example, consider the following dataset, D_1:\n\n\n\nC\nF\n\n\n\n\nF\n0\n\n\nF\n0\n\n\nF\n0\n\n\nF\n0\n\n\nF\n0\n\n\nF\n0\n\n\nF\n0\n\n\nE\n1\n\n\n\nIn D_1, the conditional entropy of the class with respect to f is 0, since f entirely describes C. If entropy was the only metric we considered, we would surely want to use f to split D_1 further as it has the lowest entropy we can imagine. However, this would likely lead to overfitting.\nRemember the example of language recognition between French and English. We might obtain such a dataset after selecting only sentences that contain substring eau: we expect this trigram to characterise the French language, but for some reason we have one funny English sentence that contains it as well. Maybe f is a very specific feature which applies to the English sentence only (for example, containing exactly 12 occurrences of a and 7 occurrences of p,). It would then be totally irrelevant to filter the dataset based on the value of f — despite its low entropy.\nHowever, since the original dataset already had a rather low entropy (it almost consisted of French words only), subtracting 0 to the initial entropy won’t suffice to make the information gain very high.\nWe’ll then be able to decide to stop training a specific branch as soon as the highest information gain over all the features we consider is lower than a certain threshold."
  },
  {
    "objectID": "posts/decision_trees.html#training-for-classification",
    "href": "posts/decision_trees.html#training-for-classification",
    "title": "A comprehensive introduction to decision trees",
    "section": "2.3 Training for classification",
    "text": "2.3 Training for classification\nThe algorithm we’ll use to train decision trees is based on the following principle: at every node, we split the dataset according to a decision d such that the information gain of the class relative to d is maximised over all decisions. To implement it, we’ll start by defining a class for every possible decision. For our purposes, there will be two classes: MatchClass and SplitAroundLinearForm.\nMatchClass takes the index of one feature and creates one subtree for each of the finitely-many possible values it can take.\nSplitAroundLinearForm takes indices of features, associated coefficients and a pivot p. The indices and coefficients define a linear form: \\ell(X) = \\sum_{i \\in I} c_i f_i(X), where I is the set of indices and (c_i) are the coefficients. This decision creates two subtrees, one for the data points X that satisfy \\ell(X) \\le p and one for the others.\nEach of these classes will have two attributes and three methods:\n\nrange_size: int is the number of subtrees created by the decision;\ndataset: numpy.ndarray is the dataset taken by the node;\ninfer takes one input and returns the index of the subtree the input would be sent to;\ninformation_gain returns the information gain of the class relative to the decision;\nsplit_dataset returns a sequence of datasets such that the i-th element of the sequence consists of the data points that are sent to the i-th subtree of the decision.\n\nWe’ll represent datasets as Numpy arrays of shape (N, p + 1), where N is the number of data points and p is the number of features. The first column indicates the class; which means that the first feature is represented by index 1, not 0.\nFor inference, inputs are represented as arrays of shape (p,) since their class is unknown.\nWith that in mind, we can create a base class for decisions, as follows:\nfrom abc import ABC\n\nclass Decision(ABC):\n    def __init__(self, range_size, dataset: np.ndarray):\n        \"\"\"\n        :param range_size: number of possible outcomes (e.g. 2 for splitting around a pivot, k for features with k\n        classes)\n        :param dataset: two-dimensional numpy array. dataset[i][j] is the j-th feature of the i-th data point and\n        dataset[i][0] is its class.\n        \"\"\"\n        self.range_size = range_size\n        self.dataset = dataset\n        super(Decision, self).__init__()\n\n    @abc.abstractmethod\n    def infer(self, x: np.ndarray) -&gt; int:\n        \"\"\"\n        Determines the branch to follow given an input.\n        :param x: input represented as a one-dimensional numpy array.\n        :return: index of the branch that matches the input.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def information_gain(self) -&gt; float:\n        \"\"\"\n        Computes the information gain of the decision on the class of the node.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def filter_dataset(self):\n        pass\nMatchClass has the following implementation:\nclass MatchClass(Decision):\n    def __init__(self, feature_index: int, dataset):\n        \"\"\"\n        :param feature_index: feature_index is the BASE-1 (!!!!!) index of the feature in the datasets.\n        It is assumed that the set of values taken by the feature is a range 0, ..., n.\n        \"\"\"\n        range_size = int(np.max(dataset[:, feature_index])) + 1\n        self.feature_index: int = feature_index\n\n        super(MatchClass, self).__init__(range_size, dataset)\n\n    def __repr__(self):\n        return f\"&lt;Match class: class: {self.feature_index}&gt;\"\n\n    def infer(self, x):\n        return int(x[self.feature_index - 1])\n\n    def information_gain(self) -&gt; float:\n        return calculate_information_gain(self.dataset[:, [self.feature_index, 0]])\n\n    def filter_dataset(self):\n        for i in range(self.range_size):\n            yield self.dataset[self.dataset[:, self.feature_index] == i, :]\nFor simplicity, we added restrictions on the representation of the range of discrete features: it has to be a contiguous range of integers starting at 0. We also assumed that the highest possible value is present in the dataset.\nThe information gain is computed using the function we defined earlier. To satisfy the format required by calculate_information_gain, we select, in this order, the feature column and the class column of the dataset to pass it to the function.\nTo implement SplitLinearFormAroundPivot, we introduce the method make_discretised. It creates a dataset with an additional binary feature that tells whether the linear form evaluates to a value less than or equal to the pivot, or greater than the pivot.\nclass SplitLinearFormAroundPivotDecision(Decision):\n    # Function to calculate discretised.\n    def __init__(self, feature_indices: np.ndarray, weights: np.ndarray, pivot: float, dataset: np.ndarray):\n        \"\"\"\n        :param feature_indices: List of (base-1) indices of features that contribute to the linear form. No need to\n        specify features with a zero coefficient.\n        :param weights: Coefficient of each of the features specified in the feature_indices list.\n        :param pivot:\n        \"\"\"\n        super().__init__(2, dataset)\n        self.pivot: float = pivot\n        self.weights = weights\n        self.feature_indices = feature_indices\n\n    def make_discretised(self):\n        number_of_examples, _ = self.dataset.shape\n        # discretised is a 2-dimensional numpy array with two columns.\n        # First step: the first one contains the linear combinations and the second one contains the classes.\n        discretised = np.empty((number_of_examples, 2))\n        discretised[:, 0] = self.dataset[:, self.feature_indices] @ self.weights\n        discretised[:, 1] = self.dataset[:, 0]\n\n        # We turn the continuous conditioning variable into a binary one. Value 0 indicates that the value of the\n        # linear combination is less than or equal to the pivot, value 1 indicates it is strictly greater than the\n        # pivot.\n        discretised[:, 0] = (discretised[:, 0] &lt;= self.pivot) * 1\n        return discretised\nThis allows to easily define the required methods:\ndef infer(self, x):\n    return 0 if x[self.feature_indices - 1] @ self.weights &lt; self.pivot else 1\n\ndef information_gain(self) -&gt; float:\n    return calculate_information_gain(self.make_discretised())\n\ndef filter_dataset(self):\n    leq_indices = self.make_discretised()[:, 0] == 0\n    return self.dataset[leq_indices, :], self.dataset[~leq_indices, :]"
  },
  {
    "objectID": "posts/decision_trees.html#training-for-regression",
    "href": "posts/decision_trees.html#training-for-regression",
    "title": "A comprehensive introduction to decision trees",
    "section": "3.2 Training for regression",
    "text": "3.2 Training for regression"
  },
  {
    "objectID": "posts/convo1.html",
    "href": "posts/convo1.html",
    "title": "Build a convolutional neural network from scratch",
    "section": "",
    "text": "1 Introduction\nThis article introduces convolutional networks and shows how to implement one from scratch in Python. Here are the topics we’ll cover:\n\nDefinition of cross-correlation.\nTypical convolutional layers.\nPooling.\nDerivation of the differential of convolutional layers for gradient descent.\nAlexNet.\n\n\n\n2 Cross-correlation"
  },
  {
    "objectID": "posts/max_likelihood.html#application-to-regression-models",
    "href": "posts/max_likelihood.html#application-to-regression-models",
    "title": "An introduction to maximum likelihood estimation and applications to machine learning",
    "section": "2.1 Application to regression models",
    "text": "2.1 Application to regression models\nRegression models seek to predict a real number y based on an observation x. To translate this definition into our probabilistic framework, we’ll assume that given an observation x, possible predictions are uniformly distributed around some mean \\mu with some standard deviation \\sigma^2. For simplicity, we’ll assume that the standard deviation is fixed across all inputs, and that it equals 1. Our goal is therefore to approximate \\mu. The most probable outcome when making a normally-distributed draw over \\R being the mean of the distribution, \\mu corresponds to what the model would predict in a single-prediction framework.\nSuppose you have a dataset D = \\{(x^{(i)}, y^{(i)}) : 1 \\le i \\le N\\} where x^{(i)} \\in U are inputs, lying in some space U, and y^{(i)} \\in \\R are predicted values. We describe our model by a function f_W : U \\to \\R that depends on a parameter W. The likelihood of the dataset given some parameter W is:\nL(\\theta) = \\prod_{i = 1}^N \\alpha \\exp\\left(-\\frac12(y^{(i)}-f_{W}(x^{(i)}))^2\\right),\nwhere \\alpha is a positive constant.\nThe \\log-likelihood is then: -\\frac12 \\sum_{i = 1}^N (f_W(x^{(i)}) - y^{(i)})^2 + \\text{ stuff}.\nIn machine learning, we often consider cost functions that we seek to minimise, so we actually want to minimise: \\sum_{i = 1}^N (f_W(x^{(i)}) - y^{(i)})^2.\nGrouping terms together, we show that if the output contains multiple units (in other words, if we want to find a normally-distributed vector), the cost function becomes: \\sum_{i = 1}^N \\|f_W(x^{(i)}) - y^{(i)}\\|^2, where \\|\\cdot\\| is the L^2 norm."
  },
  {
    "objectID": "posts/max_likelihood.html#application-to-binary-classification",
    "href": "posts/max_likelihood.html#application-to-binary-classification",
    "title": "An introduction to maximum likelihood estimation and applications to machine learning",
    "section": "2.2 Application to binary classification",
    "text": "2.2 Application to binary classification\nWe now suppose that our dataset D contains points (x^{(i)}, y^{(i)}) \\in U \\times \\{0, 1\\} for 1 \\le i \\le N, where U is an arbitrary input space. The model is described by a function f_W: U \\to (0, 1) that assigns an input to the probability of the associated output being 1, thus defining a Bernoulli distribution. The logarithm of the probability of k \\in \\{0, 1\\} for parameter W given input x^{(i)} is therefore k \\log\\left(f_W(x^{(i)})\\right) + (1-k)\\log\\left(1 - f_W(x^{(i)})\\right). The \\log-likelihood is: \\sum_{i = 1}^N y^{(i)} \\log(f_W(x^{(i)})) + (1-y^{(i)})\\log\\left(1 - f_W(x^{(i)})\\right).\nand the cost function we want to minimise is: \\sum_{i = 1}^N y^{(i)} \\log\\left(f_W(x^{(i)})\\right) + (1-y^{(i)})\\log\\left(1 - f_W(x^{(i)})\\right)."
  },
  {
    "objectID": "posts/max_likelihood.html#application-to-classification",
    "href": "posts/max_likelihood.html#application-to-classification",
    "title": "An introduction to maximum likelihood estimation and applications to machine learning",
    "section": "2.2 Application to classification",
    "text": "2.2 Application to classification\nWe now suppose that our dataset D contains points (x^{(i)}, y^{(i)}) \\in U \\times \\{0, 1\\} for 1 \\le i \\le N, where U is an arbitrary input space. The model is described by a function f_W: U \\to (0, 1) that assigns an input to the probability of the associated output being 1, thus defining a Bernoulli distribution. The logarithm of the probability of k \\in \\{0, 1\\} for parameter W given input x^{(i)} is therefore k \\log\\left(f_W(x^{(i)})\\right) + (1-k)\\log\\left(1 - f_W(x^{(i)})\\right). The \\log-likelihood is: \\sum_{i = 1}^N y^{(i)} \\log\\left(f_W(x^{(i)})\\right) + (1-y^{(i)})\\log\\left(1 - f_W(x^{(i)})\\right).\nand the cost function we want to minimise is: -\\sum_{i = 1}^N y^{(i)} \\log\\left(f_W(x^{(i)})\\right) + (1-y^{(i)})\\log\\left(1 - f_W(x^{(i)})\\right).\nWe can generalise this formula to any classification problem. Consider a set of k classes (without loss of generality, we’ll assume classes are \\{1, \\mathellipsis, k\\}). Instead of computing a single real number, f_W will now compute a vector of k positive numbers adding up to 1. For all i \\in \\{1, \\mathellipsis, k\\}, the i-th component of the output of f_W is an estimation of the probability of class i. This naturally defines a probability distribution over the set of all classes.\nGiven an input x^{(i)}, the \\log-likelihood of output y^{(i)} for parameter W is then: \\log f_W(x^{(i)})_y^{(i)},\nand the cost of the dataset is: -\\sum_{i = 1}^N \\log f_W(x^{(i)})_{y^{(i)}}."
  },
  {
    "objectID": "posts/vector_calculus.html#partial-derivatives-and-total-derivatives",
    "href": "posts/vector_calculus.html#partial-derivatives-and-total-derivatives",
    "title": "Multivariate calculus for machine learning: from theory to building an automatic differentiation system",
    "section": "2.2 Partial derivatives and total derivatives",
    "text": "2.2 Partial derivatives and total derivatives\nIt can be shown that if a function is differentiable at point x_0 then it admits an i-th partial derivative at x_0, for every i. The converse is not true, however it is sufficient to admit continuous partial derivatives along every component at some point x_0 to be differentiable at x_0. Moreover, the i-th partial derivative of f at x_0 is given by D_{x_0}f(e_i). This means that if V is finitely generated as well then the matrix of D_{x_0}f is (\\partial_j f_i(x_0))_{1 \\le i \\le \\dim(V), 1 \\le j \\le n}. This matrix is known as the Jacobian matrix (of f, at point x_0), that we’ll denote [D_{x_0}f] (or [D_{x_0}f]_{(e_i)} if the choice of the canonical basis is not trivial), or J_{x_0}(f). It can be computed from the functional representation of the differential by plugging in each vector of a basis of the domain.\nIt follows from the calculation rules we saw in the first section that the Jacobian matrix of a linear combination of two functions is the linear combination of their Jacobians, and that the Jacobian of the composition g \\circ f at point x_0 is: [D_{x_0}(g\\circ f)] = [D_{f(x_0)}g] [D_{x_0}f], where juxtaposition denotes matrix multiplication. This formula allows us to express the partial derivatives of"
  },
  {
    "objectID": "posts/vector_calculus.html#partial-derivatives-and-total-differentials",
    "href": "posts/vector_calculus.html#partial-derivatives-and-total-differentials",
    "title": "Multivariate calculus for machine learning: from theory to building an automatic differentiation system",
    "section": "2.2 Partial derivatives and total differentials",
    "text": "2.2 Partial derivatives and total differentials\nIt can be shown that if a function is differentiable at point x_0 then it admits an i-th partial derivative at x_0, for every i. The converse is not true, however it is sufficient to admit continuous partial derivatives along every component at some point x_0 to be differentiable at x_0. Moreover, the i-th partial derivative of f at x_0 is given by D_{x_0}f(e_i). This means that if V is finitely generated as well then the matrix of D_{x_0}f is (\\partial_j f_i(x_0))_{1 \\le i \\le \\dim(V), 1 \\le j \\le n}. This matrix is known as the Jacobian matrix (of f, at point x_0), that we’ll denote [D_{x_0}f] (or [D_{x_0}f]_{(e_i)} if the choice of the canonical basis is not trivial), or J_{x_0}(f). It can be computed from the functional representation of the differential by plugging in each vector of a basis of the domain.\nIt follows from the calculation rules we saw in the first section that the Jacobian matrix of a linear combination of two functions is the linear combination of their Jacobians, and that the Jacobian of the composition g \\circ f at point x_0 is: [D_{x_0}(g\\circ f)] = [D_{f(x_0)}g] [D_{x_0}f], where juxtaposition denotes matrix multiplication."
  },
  {
    "objectID": "posts/vector_calculus.html#differentiating-a-scalar-valued-function",
    "href": "posts/vector_calculus.html#differentiating-a-scalar-valued-function",
    "title": "Multivariate calculus for machine learning: from theory to building an automatic differentiation system",
    "section": "2.3 Differentiating a scalar-valued function",
    "text": "2.3 Differentiating a scalar-valued function\nIn machine learning, we can content ourselves with scalar- (and even, real-) valued functions as the final output we want to differentiate is the cost incurred by a dataset. We model the computation of the cost function as the composition C \\circ M of the functions C : V \\to \\R and M : U \\to V. U is the parameter space (as we want to differentiate the cost with respect to the parameters of the model), and V is the output space. M computes the output of the model and C computes the cost of the predictions."
  },
  {
    "objectID": "posts/vector_calculus.html#total-derivatives",
    "href": "posts/vector_calculus.html#total-derivatives",
    "title": "Multivariate calculus for machine learning: from theory to building an automatic differentiation system",
    "section": "2.1 Total derivatives",
    "text": "2.1 Total derivatives\nLet f: \\mathbb{C}^n \\to \\mathbb{C}^p. f is said to be totally differentiable, (or Fréchet-differentiable, or, for our purposes, simply differentiable), at point \\mathbf{a} \\in \\mathbb{C}^n if there exists a linear map D_{\\mathbf{a}}(f) : \\mathbb{C}^n \\to \\mathbb{C}^p such that:\n\\begin{align*}\n\\mathbb{C}^n \\setminus \\{0\\} &\\to \\mathbb{C}^p\\\\\n\\boldsymbol{\\delta} &\\mapsto \\frac{f(\\mathbf{a} + \\boldsymbol{\\delta}) - f(\\mathbf{a}) - D_{\\mathbf{a}}(f)(\\boldsymbol{\\delta})}{\\|\\boldsymbol{\\delta}\\|}\n\\end{align*}\ntends to 0 at 0.\n\n\n\n\n\n\nNote\n\n\n\nIt is actually useful to use complex numbers, even when focusing on applications to machine learning. For example, convolutional layers may compute the Fourier transform of a value that depends on parameters, which requires parameters to be applied to complex-valued functions.\n\n\nA few comments:\n\nIf f is differentiable at point \\mathbf{a} then it can be shown that there exists a unique map D_{\\mathbf{a}}(f) that satisfies the above conditions. It is called the total derivative, or (Fréchet-) differential of f at point \\mathbf{a}.\nThe definition assumes the choice of a norm over each of the two spaces. If a vector space over \\mathbb{C} is finitely generated then all norms on this space define the same limits of functions, so you may choose whichever norm you fancy the most.\nThis definition generalises the concept of differentiability and derivatives for real-valued functions, in the sense that f : \\mathbb{C} \\to \\mathbb{R} has a derivative f'(a) at point a if and only if it is differentiable at a (as per the above definition, when viewed as a function from \\mathbb{C}^1 to \\mathbb{C}^1) with differential \\delta \\mapsto f'(a)\\delta.\n\nThis definition can be easily generalised to functions that map a subset of any vector space over \\mathbb{R} or \\mathbb{C} to another vector space over the same field. The only non-trivial difference is that we’ll then require D_\\mathbf{a}(f) to be continuous (this assumption was unnecessary in the previous setting because linear maps defined on a finitely-generated real or complex normed vector space are always continuous).\nHere are some examples of differentiable functions.\n\nEvery continuous linear map is differentiable everywhere, and its differential at any point is the linear map itself.\nIf \\phi is linear and f is differentiable at \\mathbf{a}, then \\phi \\circ f is differentiable at \\mathbf{a} and its differential is \\phi \\circ D_{\\mathbf{a}}f\nLet g: \\mathbb{C}^p \\to \\mathbb{C}^s. We assume f is differentiable at point \\mathbf{a} and g is differentiable at f(\\mathbf{a}). Then g \\circ f is differentiable at \\mathbf{a} and D_\\mathbf{a}(g \\circ f) = D_{f(\\mathbf{a})}(g) \\circ D_\\mathbf{a}(f)."
  },
  {
    "objectID": "posts/vector_calculus.html#partial-derivatives",
    "href": "posts/vector_calculus.html#partial-derivatives",
    "title": "Multivariate calculus for machine learning: from theory to building an automatic differentiation system",
    "section": "2.2 Partial derivatives",
    "text": "2.2 Partial derivatives\nAnother approach to differentiation of multivariate functions involves applying the one-dimensional definition of differentiability and derivatives to partial functions x_i \\mapsto f(x_1, \\mathellipsis, x_i, \\mathellipsis, x_n). This motivates the definition of partial derivatives. Given f: \\mathbb{C}^n \\to \\mathbb{C}^p and i \\in \\{1, \\mathellipsis, n\\}, the i-th partial derivative of f at point \\mathbf{a} is the function that maps \\lambda \\in \\mathbb{C} to the limit of \\frac{f(\\mathbf{a} + \\lambda \\mathbf{e_i}) - f(\\mathbf{a})}{\\lambda} as \\lambda \\to 0, if there is one. Here \\mathbf{e_i} denotes the i-th vector of the canonical basis of \\mathbb{C}^n, i.e. the vector whose components are all 0 except for the i-th one, which is 1.\nThe i-th partial derivative of f is denoted \\partial_if. It is sometimes convenient to work with formal expressions rather than functions, in which case we write \\frac{\\partial f(x, y)}{\\partial x}.\nIt can be shown that if a function is differentiable at point a then it admits an i-th partial derivative at a, for every i. The converse is not true, however it is sufficient to admit continuous partial derivatives along every component at some point a to be differentiable at a. Moreover, the i-th partial derivative of f at a is given by D_{a}(f)(e_i). This means that the matrix of D_{a}(f) is (\\partial_j f_i(a))_{1 \\le i \\le p, 1 \\le j \\le n}. This matrix is known as the Jacobian matrix (of f, at point a).\nIt follows from the formula for the total derivative of the composition of two functions that the Jacobian matrix of g \\circ f is J_g J_f, where J_g (respectively J_f) is the Jacobian matrix of g (respectively f). Expanding this formula gives:\n\\partial_j(g_i \\circ f)(a) = \\sum_{k = 1}^p \\partial_k g_i(f(a)) \\partial_j f_k(a), where g: \\mathbb C^n \\to \\mathbb C^p and f: \\mathbb C^p \\to \\mathbb C^s."
  },
  {
    "objectID": "posts/vector_calculus.html#laying-the-bricks",
    "href": "posts/vector_calculus.html#laying-the-bricks",
    "title": "Multivariate calculus for machine learning: from theory to building an automatic differentiation system",
    "section": "3.1 Laying the bricks",
    "text": "3.1 Laying the bricks\nWe consider a set of functions whose partial derivatives are all known. These functions are the building blocks of every expression we want to differentiate. For example, they may include sum, product, negation, reciprocal and exponentiation, as well as exponentiation, logarithm and absolute value.\nThese functions are assumed to be differentiable at any point at which they’re not undefined. For example, we would either not allow absolute value to be applied to 0 at all or arbitrarily extend its derivative by choosing among -1 and 1.\nWe can express the cost of a model as follows: c = f(u(x)), where f is a scalar-valued function whose derivative is known, u = (u_1, \\mathellipsis, u_p) and x = (x_1, \\mathellipsis, x_n). Our goal is to calculate the partial derivative of c with respect to x_j (for every j \\in \\{1, \\mathellipsis, n\\}).\nFor every variable y, we let d(y) denote \\frac{\\partial c}{\\partial y}.\nIt follows from the formula we saw at the end of the previous section that for j \\in \\{1, \\mathellipsis, n\\}: d(x_j) = \\sum_{k = 1}^p d(u_k) \\frac{\\partial u_k(x)}{\\partial x_j}. For every j \\in \\{1, \\mathellipsis, p\\}, (d(u_k)) is known and we then have to apply the same reasoning to the expression u_k(x) (which has to be of the form g(v(x)), for some g whose derivative is known)."
  },
  {
    "objectID": "posts/vector_calculus.html#implementation",
    "href": "posts/vector_calculus.html#implementation",
    "title": "Multivariate calculus for machine learning: from theory to building an automatic differentiation system",
    "section": "2.2 Implementation",
    "text": "2.2 Implementation"
  },
  {
    "objectID": "posts/vector_calculus.html#implementation-outline",
    "href": "posts/vector_calculus.html#implementation-outline",
    "title": "Multivariate calculus for machine learning: from theory to building an automatic differentiation system",
    "section": "3.2 Implementation outline",
    "text": "3.2 Implementation outline\nWe’ll define a class that represents variables and expressions of the form f(E), where f is a function whose derivative is known and E is an expression or a variable. Each instance of the class will have three fields:\n\ntype (e.g. Variable, Sum, Product, NaturalLog…):\nvalue (a complex number);\nderivative, a pointer to its derivative.\n\nNon-atomic expressions (i.e. expressions that are not variables) also contain references to their sub-expressions (for example, expressions of type NaturalLog have one sub-expression and expressions of type Sum have two). They might as well contain scalar parameters. For example, we can have a type AffineCombination that describes expressions of the form aE + b where a and b are scalars and E is an expression. We won’t implement this feature in this article.\nAt the end of the differentiation procedure, the value derivative points to is the value of the derivative of the top-level expression (i.e. the cost) with respect to the current instance. In other words, it is the sum over all its super-expressions f(E) of the derivative of the cost with respect to f(E) times the derivative of f(E) with respect to E.\nTo obtain the final value of derivative, we’ll use the method exp.differentiate(h). It is called by every super-expression u(exp) of exp. The parameter h u(exp) passes to exp.differentiate is the value of the corresponding term in the expression of d(exp), i.e. d(u) \\frac{\\partial u(\\text{exp})}{\\partial \\text{exp}}. differentiate will then add h to the derivative of the current class and call sub.differentiate on each of its sub-expressions. (Don’t worry if this sounds confusing, it’ll be easier to understand in the implementation section.)\nThe derivative of each expression involved in the computation of the cost will then be calculated from the top-level expression (whose differentiate method takes parameter 1) down to each variables."
  },
  {
    "objectID": "posts/vector_calculus.html#the-expression-class",
    "href": "posts/vector_calculus.html#the-expression-class",
    "title": "Multivariate calculus for machine learning: from theory to building an automatic differentiation system",
    "section": "4.1 The Expression class",
    "text": "4.1 The Expression class\nThis subsection decribes the general implementation of the Expression class. In many aspects, implementation details shown here are a matter of personal style. Feel free to skip to the next section, where we’ll discuss the implementation of differentiate.\nHere is the content of our header file.\nenum Type {\n    Sum,\n    Product,\n    NaturalLog,\n    Variable,\n};\n\ntypedef std::complex&lt;double&gt; Scalar;\n\nclass Expression {\nprivate:\n    Type type;\n    Scalar value;\n    std::shared_ptr&lt;Scalar&gt; derivative;\n    std::shared_ptr&lt;Expression&gt; lhs, rhs;\n\npublic:\n    Expression(Scalar value);\n\n    Expression(double value);\n\n    Expression();\n\n    Expression(const Expression& lhs, const Expression& rhs, Type type);\n\n    Expression operator+(const Expression& other) const;\n\n    Expression operator*(const Expression& other) const;\n\n    Expression operator*(Scalar scalar) const;\n\n    Expression operator-() const;\n\n    Expression operator-(const Expression& other) const;\n\n    bool operator&lt;(const Expression& other) const;\n\n    bool operator&lt;(double x) const;\n\n    bool operator&gt;(const Expression& other) const;\n\n    bool operator&gt;(double x) const;\n\n    bool operator==(const Expression& other) const;\n\n    bool operator!=(const Expression& other) const;\n\n    [[nodiscard]] Scalar getValue() const;\n\n    [[nodiscard]] Scalar getDerivative() const;\n\n    void differentiate(Scalar h) const;\n};\nHere is a brief overview of what we did:\n\nWe defined two implicit constructors that allow automatic conversions of complex and floating-point numbers into an instance of Expression.\nWe defined a constructor that takes no parameters in order to satisfy the requirements of the Eigen library to create matrices whose coefficients are instances of Expression.\nWe overloaded basic operators to perform most operations required to implement simple machine learning models."
  },
  {
    "objectID": "posts/vector_calculus.html#the-differentiate-method",
    "href": "posts/vector_calculus.html#the-differentiate-method",
    "title": "Multivariate calculus for machine learning: from theory to building an automatic differentiation system",
    "section": "4.2 The differentiate method",
    "text": "4.2 The differentiate method\nHere is the general structure of the differentiate method:\nvoid Expression::differentiate(Scalar h) const {\n    *(this-&gt;derivative) += h;\n    switch (type) {\n        case Sum:\n            this-&gt;lhs-&gt;differentiate(...);\n            this-&gt;rhs-&gt;differentiate(...);\n            break;\n        case Product:\n            this-&gt;lhs-&gt;differentiate(...);\n            this-&gt;rhs-&gt;differentiate(...);\n            break;\n        case NaturalLog:\n            this-&gt;lhs-&gt;differentiate(...);\n        case Variable:\n            break;\n    }\n}\nThis section derives expressions for the parameter of recursive calls of Sum, Product and NaturalLog.\n\n4.2.1 Sum\nConsider the expression z = x + y and let h denote the derivative of the cost with respect to z (i.e. d(z)), which is known (as it was passed to the differentiate method of z). We said in the implementation outline that we need to call x.differentiate(h2), where h2 is given by: h \\times \\frac{\\partial z}{\\partial x}, and similarly with y.\nWe calculate \\frac{\\partial z}{\\partial x_1} = \\frac{\\partial z}{\\partial x_2} = 1. Therefore, the Sum case should be:\ncase Sum:\n    this-&gt;lhs-&gt;differentiate(h);\n    this-&gt;rhs-&gt;differentiate(h);\n\n\n4.2.2 Product\nSimilarly, consider the expression z = x * y. The derivative of z with respect to x is y and the derivative of z with respect to y is x. Therefore, the Product case should be:\ncase Product:\n    this-&gt;lhs-&gt;differentiate(h * rhs-&gt;value());\n    this-&gt;rhs-&gt;differentiate(h * lhs-&gt;value());\n\n\n4.2.3 Natural logarithm\nFinally, let’s consider the expression z = ln(x). The derivative of z with respect to x is 1/x, which means that the value we pass to the differentiate method of x is h / x:\ncase NaturalLog:\n    this-&gt;lhs-&gt;differentiate(h / lhs-&gt;value);"
  },
  {
    "objectID": "posts/decision_trees.html#the-decision-class",
    "href": "posts/decision_trees.html#the-decision-class",
    "title": "A comprehensive introduction to decision trees",
    "section": "3.1 The Decision class",
    "text": "3.1 The Decision class\nWe’ll start by defining a class for every possible type of decision. For our purposes, there will be two classes: MatchClass and SplitAroundLinearForm.\nMatchClass takes the index of one feature and creates one subtree for each of the finitely-many possible values it can take.\nSplitAroundLinearForm takes indices of features, associated coefficients and a pivot p. The indices and coefficients define a linear form: \\ell(X) = \\sum_{i \\in I} c_i f_i(X), where I is the set of indices and (c_i) are the coefficients. This decision creates two subtrees, one for the data points X that satisfy \\ell(X) \\le p and one for the others.\nEach of these classes will have two attributes and three methods:\n\nrange_size: int is the number of subtrees created by the decision;\ndataset: numpy.ndarray is the dataset taken by the node;\ninfer takes one input and returns the index of the subtree the input would be sent to;\ninformation_gain returns the information gain of the class relative to the decision;\nsplit_dataset returns a sequence of datasets such that the i-th element of the sequence consists of the data points that are sent to the i-th subtree of the decision.\n\nWe’ll represent datasets as Numpy arrays of shape (N, p + 1), where N is the number of data points and p is the number of features. The first column indicates the class; which means that the first feature is represented by index 1, not 0.\nFor inference, inputs are represented as arrays of shape (p,) since their class is unknown.\nWith that in mind, we can create a base class for decisions, as follows:\nfrom abc import ABC\n\nclass Decision(ABC):\n    def __init__(self, range_size, dataset: np.ndarray):\n        \"\"\"\n        :param range_size: number of possible outcomes (e.g. 2 for splitting around a pivot, k for features with k\n        classes)\n        :param dataset: two-dimensional numpy array. dataset[i][j] is the j-th feature of the i-th data point and\n        dataset[i][0] is its class.\n        \"\"\"\n        self.range_size = range_size\n        self.dataset = dataset\n        super(Decision, self).__init__()\n\n    @abc.abstractmethod\n    def infer(self, x: np.ndarray) -&gt; int:\n        \"\"\"\n        Determines the branch to follow given an input.\n        :param x: input represented as a one-dimensional numpy array.\n        :return: index of the branch that matches the input.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def information_gain(self) -&gt; float:\n        \"\"\"\n        Computes the information gain of the decision on the class of the node.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def filter_dataset(self):\n        pass\nMatchClass has the following implementation:\nclass MatchClass(Decision):\n    def __init__(self, feature_index: int, dataset):\n        \"\"\"\n        :param feature_index: feature_index is the BASE-1 (!!!!!) index of the feature in the datasets.\n        It is assumed that the set of values taken by the feature is a range 0, ..., n.\n        \"\"\"\n        range_size = int(np.max(dataset[:, feature_index])) + 1\n        self.feature_index: int = feature_index\n\n        super(MatchClass, self).__init__(range_size, dataset)\n\n    def __repr__(self):\n        return f\"&lt;Match class: class: {self.feature_index}&gt;\"\n\n    def infer(self, x):\n        return int(x[self.feature_index - 1])\n\n    def information_gain(self) -&gt; float:\n        return calculate_information_gain(self.dataset[:, [self.feature_index, 0]])\n\n    def filter_dataset(self):\n        for i in range(self.range_size):\n            yield self.dataset[self.dataset[:, self.feature_index] == i, :]\nFor simplicity, we added restrictions on the representation of the range of discrete features: it has to be a contiguous range of integers starting at 0. We also assumed that the highest possible value is present in the dataset.\nThe information gain is computed using the function we defined earlier. To satisfy the format required by calculate_information_gain, we select, in this order, the feature column and the class column of the dataset to pass it to the function.\nTo implement SplitLinearFormAroundPivot, we introduce the method make_discretised. It creates a dataset with an additional binary feature that tells whether the linear form evaluates to a value less than or equal to the pivot, or greater than the pivot.\nclass SplitLinearFormAroundPivot(Decision):\n    # Function to calculate discretised.\n    def __init__(self, feature_indices: np.ndarray, weights: np.ndarray, pivot: float, dataset: np.ndarray):\n        \"\"\"\n        :param feature_indices: List of (base-1) indices of features that contribute to the linear form. No need to\n        specify features with a zero coefficient.\n        :param weights: Coefficient of each of the features specified in the feature_indices list.\n        :param pivot:\n        \"\"\"\n        self.pivot: float = pivot\n        self.weights = weights\n        self.feature_indices = feature_indices\n\n        super(SplitLinearFormAroundPivot, self).__init__(2, dataset)\n\n    def make_discretised(self):\n        number_of_examples, _ = self.dataset.shape\n        # discretised is a 2-dimensional numpy array with two columns.\n        # First step: the first one contains the linear combinations and the second one contains the classes.\n        discretised = np.empty((number_of_examples, 2))\n        discretised[:, 0] = self.dataset[:, self.feature_indices] @ self.weights\n        discretised[:, 1] = self.dataset[:, 0]\n\n        # We turn the continuous conditioning variable into a binary one. Value 0 indicates that the value of the\n        # linear combination is less than or equal to the pivot, value 1 indicates it is strictly greater than the\n        # pivot.\n        discretised[:, 0] = (discretised[:, 0] &lt;= self.pivot) * 1\n        return discretised\nThis allows to easily define the required methods:\ndef infer(self, x):\n    return 0 if x[self.feature_indices - 1] @ self.weights &lt; self.pivot else 1\n\ndef information_gain(self) -&gt; float:\n    return calculate_information_gain(self.make_discretised())\n\ndef filter_dataset(self):\n    leq_indices = self.make_discretised()[:, 0] == 0\n    return self.dataset[leq_indices, :], self.dataset[~leq_indices, :]"
  },
  {
    "objectID": "posts/decision_trees.html#choosing-the-best-decision",
    "href": "posts/decision_trees.html#choosing-the-best-decision",
    "title": "A comprehensive introduction to decision trees",
    "section": "3.2 Choosing the best decision",
    "text": "3.2 Choosing the best decision\nA first component to our training algorithm is one that selects an optimal pivot for a given linear form. It takes a list of feature indices, a list of weights and a dataset and returns an instance of SplitAroundLinearForm and the associated information gain.\nSince we’re training our model out of a finite dataset, finding an optimal pivot boils down to finding an optimal pivot over the set of midpoints between values taken by the linear form at each data point. Therefore, we’ll start by computing a set of candidates as follows:\ndef find_pivot(dataset: np.ndarray, feature_indices: List[int], weights: List[float]):\n    # Calculating the value of the linear form at each point present in the dataset.\n    x = dataset[:, feature_indices] @ np.array(weights)\n\n    # Extracting unique values.\n    v = np.unique(x)\n\n    if len(v) == 1:\n        dec = SplitLinearFormAroundPivot(feature_indices, weights, v[0], dataset)\n        return dec, dec.information_gain()\n\n    # mid-points between unique values taken by the linear form\n    candidates = v[:-1] + (v[1:] - v[:-1]) / 2\nWe then iterate over all candidates to find the pivot that maximises the information gain.\n    optimal_decision = None\n    optimal_information_gain = None\n\n    for pivot in candidates:\n        dec = SplitLinearFormAroundPivot(feature_indices, weights, pivot, dataset)\n        ig = dec.information_gain()\n        if optimal_information_gain is None or ig &gt; optimal_information_gain:\n            optimal_information_gain = ig\n            optimal_decision = dec\n\n    return optimal_decision, optimal_information_gain\nWe’ll implement the training algorithm within a TreeStructure class, which represents a node. It has the following __init__ function:\nclass TreeStructure:\n    def __init__(self, dataset: np.ndarray, feature_types: np.ndarray, depth: int = 0):\n        self.children: List[TreeStructure] = []\n        self.decision: Optional[Decision] = None\n        self.prediction: Optional[int] = None\n        self.dataset = dataset\n        self.depth = depth\n        self.feature_types = feature_types\nfeature_types is a list of tokens indicating the type of each feature, which for our purpose can be either DISCRETE or CONTINUOUS. The depth attribute is used to stop training once the tree is becoming deep enough.\nTo choose the best decision at a given tree node, we define the function choose_decision whose high-level structure is given below:\ndef choose_decision(self):\n    best_continuous_feature = None\n    best_continuous_feature_information_gain = None\n\n    # Calculating the best continous feature\n\n    best_discrete_feature = None\n    best_discrete_feature_information_gain = None\n\n    # Calculating the best discrete feature\n\n    if best_discrete_feature_information_gain &gt; best_continuous_feature_information_gain:\n        return best_discrete_feature\n    else:\n        return best_continuous_feature\nThe best discrete feature is calculated as follows:\ndiscrete_features = [i for i, t in enumerate(self.feature_types, 1) if t == DISCRETE]\nfor f in discrete_features:\n    dec = MatchClass(f, self.dataset)\n    information_gain = dec.information_gain()\n    if best_discrete_feature_information_gain is None or information_gain &gt; best_discrete_feature_information_gain:\n        best_discrete_feature_information_gain = information_gain\n        best_discrete_feature = dec\nThe first line selects the (base 1) indices of the discrete features. We then calculates the information gain induced by each feature and store the maximum.\nThe calculation of the best continuous feature is essentially the same. The main difference is that we need to construct the set of linear forms we want to consider. The following snippet shows the simplest approach, where the only linear forms are those that depend on one feature only. They allow to filter data points according to whether some feature is less than or greater than a pivot.\nlinear_forms = [([i], [1]) for i, t in enumerate(self.feature_types, 1) if t == CONTINUOUS]\nfor l in linear_forms:\n    decision, information_gain = find_pivot(self.dataset, *l)\n    if best_continuous_feature_information_gain is None or information_gain &gt; \\\n        best_continuous_feature_information_gain:\n        best_continuous_feature_information_gain = information_gain\n        best_continuous_feature = decision\nWe can also add linear forms of the form x - y which allow to compare two features.\nOnce an optimal decision is chosen, we can populate the children array. This is achieved by the fill method. It takes an optional parameter, min_ig. If the information gain of the best feature is lower than min_ig, training will stop.\ndef fill(self, min_ig=None):\n    self.decision = self.choose_decision(self.feature_types)\n\n    if min_ig is not None and min_ig &gt; self.decision.information_gain():\n        return False\n\n    for filtered_dataset in self.decision.filter_dataset():\n        self.children.append(TreeStructure(filtered_dataset, self.feature_types, self.depth + 1))\n        return True\nThe final part of our training algorithm is fill_recursively, which starts from the root, chooses a feature and repeats on its children until a stopping criterion is met. We implement it as follows:\ndef fill_recursively(self, feature_type, max_depth=None, min_ig=None):\n    if max_depth is None or self.depth &lt; max_depth:\n        if self.fill(feature_type, min_ig):\n            for child in self.children:\n                if calculate_entropy_from_dataset(child.dataset[:, 0]) != 0:\n                    child.fill_recursively(feature_type, max_depth)\nThe stopping criterion is either max_depth or min_ig. The first line checks whether the stopping criterion on depth is met. If it is not, fill is called. If the minimum information gain stopping criterion is met, fill returns False and we stop training the current branch. Otherwise, we recurse on every child.\nInference is then implemented as follows:\ndef infer(self, x):\n    if not self.children:\n        values, counts = np.unique(self.dataset, return_counts=True)\n        ind = np.argmax(counts)\n        return values[ind]\n    return self.children[self.decision.infer(x)].infer(x)\nIf the node has no children, then we select the most likely class. Otherwise, we call the infer method of the decision and recurse on the corresponding subtree."
  },
  {
    "objectID": "posts/decision_trees.html#information",
    "href": "posts/decision_trees.html#information",
    "title": "A comprehensive introduction to decision trees",
    "section": "2.2 Information",
    "text": "2.2 Information\nWhen training a decision tree on a dataset D = \\{d_1, \\mathellipsis, d_N\\}, we want to choose a decision that holds as much information as possible on the class of the data points.\nAn information function is a continuous function I : (0, 1] \\to [0, +\\infty) that satisfies the following conditions:\n\nfor all x, y, I(xy) = I(x) + I(y): if two events are independent, the information that their intersection provides is the sum of the information that each provides;\nI is strictly decreasing: if an event is less likely to happen than an another, knowing that it happened brings more information.\n\nWe can prove that information functions are exactly the functions \\log_b, where b \\in (0, 1). Equivelently, they are the functions -\\log_b where b &gt; 1. Here is an outline of a proof, given some information function I:\n\nI is a bijection from (0, 1] onto [0, +\\infty): injectivity comes from the fact that is is strictly monotonic and unboundedness comes from the fact that I(1/2^n) = nI(1/2).\nI has an inverse E : [0, +\\infty) \\to (0, 1] that satisfies E(x + y) = E(x)E(y) for all x, y.\nE is continous and agrees with the function r \\mapsto E(1)^r on [0, \\infty) \\cap \\mathbb{Q}, therefore it is the exponential function with base E(1) \\in (0, 1).\nI is therefore the inverse of \\exp_{E(1)}, so I = \\log_{E(1)}.\n\nWe then extend I to [0, 1] by defining I(0) = \\lim_{0} I = +\\infty (which preserves the two axioms)."
  },
  {
    "objectID": "posts/decision_trees.html#entropy",
    "href": "posts/decision_trees.html#entropy",
    "title": "A comprehensive introduction to decision trees",
    "section": "2.3 Entropy",
    "text": "2.3 Entropy\nLet \\mathbb P : \\mathcal P(X) \\to [0, 1] be a probability measure with a finite sample space X and let p: X \\to [0, 1] be its mass function. We define the entropy of \\mathbb P, denoted H(\\mathbb P), as the expected value of I \\circ p with respect to \\mathbb P: H(\\mathbb P) = \\sum_{x \\in X} I(p(x)) p(x). (H is a capital \\eta, and should be pronounced eta and not eitsch…)\nFor convenience, we may use H(Y) to denote the entropy of the distribution of a random variable Y.\n\n\n\n\n\n\nNote\n\n\n\nFor simplicity, we pretend that there is a unique information function as the choice of the base does not matter for our purposes. For visualisation purposes, it is convenient to chose base \\frac{1}{|X|} (i.e. I = -\\log_{|X|}) so that H takes values between 0 and 1 (more on that just below).\n\n\nHere is our implementation:\nimport numpy as np\n\ndef calculate_entropy(distribution: np.ndarray) -&gt; float:\n    \"\"\"\n    :param distribution: one-dimensional numpy array describing a probability distribution (i.e. distribution[i] is\n    the probability of the i-th outcome).\n    :return: the entropy of the distribution.\n    \"\"\"\n    indices = distribution != 0\n    return -distribution[indices] @ np.log2(distribution[indices])\nindices contains the indices of entries with non-zero probability. Restricting distribution to indices allows not to pass 0 to log2.\nThe entropy of a probability measure helps us quantify the amount of information carried by the knowledge of the outcome of a random experiment. A high entropy means that knowing the actual outcome gives a lot of information. In other words, it means that knowing the distribution wasn’t sufficient to predict what the outcome was going to be.\nWe can use Jensen’s inequality to show that the uniform distribution maximises the entropy over all distributions over any fixed finite sample space X. On the contrary, the entropy of a distribution that takes value 1 for one outcome and 0 for every other is 0.\n\n\n\n\n\n\nJensen’s inequality\n\n\n\nIf you are not familiar with Jensen’s inequality and want to attempt the proof, it says that if Z: \\Omega \\to \\mathbb R is a random variable and f: \\R \\to \\R is a convex function then f(\\mathbb E[Z]) \\le \\mathbb E[f \\circ Z].\n\n\nAn important example to remember is the one of a Bernoulli distibution, shown below. \nThe graph shows the entropy of a Bernoulli distribution as a function of its parameter p \\in [0, 1]. If p is close to 0 or 1 then the entropy is low, as almost all the information is carried by the theoretical distribution rather than the knowledge of the actual outcome. If p is close to 1/2, it is difficult to predict what the outcome will be by just looking at the distribution and most of the information is carried by the knowledge of the outcome of the experiment."
  },
  {
    "objectID": "posts/decision_trees.html#conditional-entropy-and-information-gain",
    "href": "posts/decision_trees.html#conditional-entropy-and-information-gain",
    "title": "A comprehensive introduction to decision trees",
    "section": "2.4 Conditional entropy and information gain",
    "text": "2.4 Conditional entropy and information gain\nLet X be a random variable onto a finite domain (which we’ll assume without loss of generality to be \\{1, \\mathellipsis, n\\} for some positive integer n) and let A be an event. We define the conditional entropy of X given A as the entropy of the conditional distribution of the value of X given A, i.e.: H(X | A) = \\sum_{k = 1}^n \\mathbb P(X = k | A) I(\\mathbb P(X = k | A)).\nA high conditional entropy means that despite knowing that A occurred, we couldn’t predict what value X was going to take. A low conditional entropy means that the assumption that A occurred was enough to make the distribution of X “almost deterministic”.\nWe can then define the conditional entropy of X given another random variables Y taking values in another finite set, let’s say \\{1, \\mathellipsis, p\\} as: H(X | Y) = \\mathbb E_{y \\sim \\mathbb P_Y}[H(X | Y = y)] = \\sum_{y = 1}^p \\mathbb P(Y = y) H(X | Y = y).\nIf H(X | Y) is high, knowing what value was taken by Y won’t help us determine the value that X will take. If H(X | Y) is low, it is enough to know the value taken by Y to be able to predict what value X will take with reasonable confidence.\nTo calculate the conditional distribution of some variable X given another variable Y, we need to know their joint distribution (i.e. the distribution of the random variable (X, Y)). In practice, the information we initially have on the joint distribution is a dataset, i.e. a list of entries, each having two numerical values indicating the value of X and the value of Y. Here is how we’ll compute it:\nCONDITIONING_VARIABLE = 0\nCONDITIONED_VARIABLE = 1\n\ndef calculate_empirical_distribution(dataset: np.ndarray, return_domain: bool = False) -&gt; np.ndarray | Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Calculates the empirical distribution defined by a dataset, i.e. such that the probability of x is the number of\n    occurrences of x in the dataset divided by the total number of elements in the dataset.\n    :param dataset: numpy array.\n    :param return_domain: default to False. If True, returns a tuple (domain, distribution) where domain is\n    the same shape as distribution and distribution[i] is the probability of domain[i].\n    :return: if return_conditioning_domain is set to False: a permutation of the empirical distribution;\n    otherwise: a tuple (domain, distribution) where domain is the same shape as distribution and distribution[i] is\n    the probability of domain[i].\n    \"\"\"\n    n, = dataset.shape\n    domain, unnormalised_distribution = np.unique(dataset, axis=0, return_counts=True)\n    if return_domain:\n        return domain, unnormalised_distribution / n\n\n    return unnormalised_distribution / n\n\ndef calculate_conditional_entropy_from_dataset(dataset: np.ndarray) -&gt; float:\n    \"\"\"\n    Calculate the entropy of the conditioned variable given the value of the conditioning variable.\n    :param dataset: two-dimensional numpy array. Each row describes a data point. The first column denotes the\n    conditioning variable and the second column denotes the conditioned variable.\n    assert dataset.ndim == 2\n    assert dataset.shape[1] == 2\n    :return: conditional entropy\n    \"\"\"\n    assert dataset.ndim == 2\n    assert dataset.shape[1] == 2\n\n    n, _ = dataset.shape  # number of entries in the dataset\n    conditioning_domain, conditioning_distribution = calculate_empirical_distribution(dataset[:, CONDITIONING_VARIABLE],\n                                                                                      return_domain=True)\n    conditioned_variable_domain_cardinality, = np.unique(dataset[:, CONDITIONED_VARIABLE]).shape\n\n    # conditional_entropies[i] is the entropy of the conditioned variable for rows where the conditioning variable\n    # takes value conditioning_domain[i].\n\n    conditioning_domain_cardinality, = conditioning_domain.shape\n\n    conditional_entropies = np.zeros(conditioning_domain_cardinality, dtype=float)\n    for i in range(conditioning_domain_cardinality):\n        conditioned_dataset = dataset[dataset[:, CONDITIONING_VARIABLE] == conditioning_domain[i]]\n        conditional_entropies[i] = calculate_entropy_from_dataset(conditioned_dataset[:, CONDITIONED_VARIABLE])\n    return conditioning_distribution @ conditional_entropies\nWe first created a utility function calculate_empirical_distribution that computes the distribution of values present in a dataset.\n\n\n\n\n\n\nA different formula\n\n\n\nIf A \\in \\R^{1, p} is the row matrix whose y-th entry is \\mathbb P(Y = y) and B \\in \\R^{p, n} is the matrix whose (y, x)-th entry is \\mathbb P(X = x | Y = y) then the formula for the conditional entropy of X given Y is: H(X | Y) = \\sum_{x = 1}^n (A \\times (B \\odot I(B)))_x, where \\odot denotes the Hadamard product, i.e. cell-wise multiplication.\n\n\nFinally, we use the information gain of X relative to Y, \\text{IG}(X, Y) = H(X) - H(X | Y) to quantify the information brought by the knowledge of the value taken by Y to predict the value that X will take.\ndef calculate_information_gain(dataset: np.ndarray) -&gt; float:\n    \"\"\"\n    Calculate the information gain of the conditioning variable on the conditioned variable.\n    :param dataset: two-dimensional numpy array. Each row describes a data point. The first column denotes the\n    conditioning variable and the second column denotes the conditioned variable.\n    :return: information gain\n    \"\"\"\n    prior_entropy = calculate_entropy_from_dataset(dataset[:, CONDITIONED_VARIABLE])\n    posterior_entropy = calculate_conditional_entropy_from_dataset(dataset)\n    return prior_entropy - posterior_entropy\n\n\n\n\n\n\nNote\n\n\n\nMaximum purity is reached where the information gain is the highest, or equivalently where conditional entropy is the lowest. Therefore, you might be wondering why we should bother compute the information gain when we could simply try to minimise the entropy. The reason is that the information gain helps us decide when we should stop training.\nFor example, consider the following dataset, D_1:\n\n\n\nC\nF\n\n\n\n\nF\n0\n\n\nF\n0\n\n\nF\n0\n\n\nF\n0\n\n\nF\n0\n\n\nF\n0\n\n\nF\n0\n\n\nE\n1\n\n\n\nIn D_1, the conditional entropy of the class with respect to F is 0, since F entirely describes C. If entropy was the only metric we considered, we would surely want to use F to split D_1 further as it has the lowest entropy we can imagine. However, this would likely lead to overfitting.\nRemember the example of language recognition between French and English. We might obtain such a dataset after selecting only sentences that contain substring eau: we expect this trigram to characterise the French language, but for some reason we have one funny English sentence that contains it as well. Maybe F is a very specific feature which applies to the English sentence only (for example, containing exactly 12 occurrences of a and 7 occurrences of p,). It would then be totally irrelevant to filter the dataset based on the value of F — despite its low entropy.\nHowever, since the original dataset already had a rather low entropy (it almost consisted of French words only), subtracting 0 to the initial entropy won’t suffice to make the information gain very high.\nWe’ll then be able to decide to stop training a specific branch as soon as the highest information gain over all the features we consider is lower than a certain threshold."
  },
  {
    "objectID": "posts/pca.html",
    "href": "posts/pca.html",
    "title": "An optimisation approach to Principal Component Analysis",
    "section": "",
    "text": "Principal component analysis (PCA) is one of the most widely used dimensionality reduction algorithms. Given a dataset \\{x^{(1)}, \\mathellipsis, x^{(n)}\\} with points in \\mathbb R^d and an integer k &lt; d, its goal is to find a sub-vector space F \\sube \\mathbb R^d of dimension k and points \\{y^{(1)}, \\mathellipsis, y^{(n)}\\} \\sube F that are the closest to \\{x^{(1)}, \\mathellipsis, x^{(n)}\\}.\nThis article introduces the theory behind PCA, including pre-Hilbert spaces, orthogonal projections and Lagrange multipliers. It then describes an algorithm to compute it and shows how to implement it in Python."
  },
  {
    "objectID": "posts/pca.html#scalar-products-and-norms",
    "href": "posts/pca.html#scalar-products-and-norms",
    "title": "An optimisation approach to Principal Component Analysis",
    "section": "1.1 Scalar products and norms",
    "text": "1.1 Scalar products and norms\nA scalar product is a map \\langle \\cdot, \\cdot \\rangle : U \\times U \\to \\mathbb R that satisfies the following properties:\n\nfor all x, y \\in U, \\langle x, y \\rangle = \\langle y, x \\rangle;\nfor all x, y, z \\in U and \\lambda \\in \\mathbb R, \\langle x, y + \\lambda z \\rangle = \\langle x, y\\rangle + \\lambda \\langle x, z \\rangle;\nfor all x \\in U, \\langle x, x \\rangle \\ge 0 and if x \\neq 0, \\langle x, x \\rangle \\neq 0.\n\nIt follows from these properties that \\langle 0, 0 \\rangle = 0.\nThe most widely known example of a scalar product is the dot product on \\mathbb R^n, which given two vectors (x_i) and (y_i) returns \\sum_{i = 1}^n x_i y_i.\n\n\n\n\n\n\nNote\n\n\n\nScalar products are often defined on complex vector spaces. For completeness, given a complex vector space V, a scalar product on V is a map from V \\times V to \\mathbb C that satisfies the second and third axioms above, and such that \\langle x, y\\rangle is the complex conjugate of \\langle y, x \\rangle, for all vectors x, y \\in V.\n\n\nGiven a scalar product on a real vector space U, we can define a norm \\|\\cdot\\| by \\|x\\| = \\sqrt{\\langle x, x\\rangle}. For example, the dot product on \\mathbb R^n induces the euclidean norm, \\|x\\| = \\sqrt{\\sum x_i^2}.\nA vector space U equipped with a norm induced by a scalar product is called a pre-Hilbert space.\n\n\n\n\n\n\nNote\n\n\n\nA Hilbert space is a complete pre-Hilbert space, i.e. one where all Cauchy sequence converge!\n\n\nGiven a pre-Hilbert space (U, \\|\\cdot\\|), there exists a unique scalar product that induces \\|\\cdot\\|, so we may as well refer to (U, \\|\\cdot\\|) as (U, \\langle \\cdot, \\cdot \\rangle).\n\n\n\n\n\n\nFrom norm to scalar product\n\n\n\nIf \\langle \\cdot, \\cdot \\rangle is a scalar product then for every x, y, the following equality holds: \\langle x, y\\rangle = \\frac12\\left(\\|x + y\\|^2 - (\\|x\\|^2 + \\|y\\|^2)\\right)."
  },
  {
    "objectID": "posts/pca.html#orthonormal-families-of-vectors",
    "href": "posts/pca.html#orthonormal-families-of-vectors",
    "title": "An optimisation approach to Principal Component Analysis",
    "section": "1.2 Orthonormal families of vectors",
    "text": "1.2 Orthonormal families of vectors\nLet U be a real vector space and \\langle \\cdot, \\cdot \\rangle be a scalar product on U. Two vectors x and y are said to be orthogonal if \\langle x, y\\rangle = 0. A family (x_i)_{i \\in I} of vectors is said to be orthogonal if vectors are pairwise orthogonal, i.e. i \\neq j \\implies \\langle x_i, x_j\\rangle = 0. It is said to be orthonormal if it is orthogonal and for every i, \\|x_i\\| = 1.\nAn important result is that every finitely-generated vector space has an orthonormal basis.\n\n\n\n\n\n\nGram-Schmidt orthogonalisation process\n\n\n\nThe existence of an orthonormal basis can be proven algorithmically via the Gram-Schmidt orthogonalisation process, which turns an arbitrary basis into an orthogonal one.\n\n\nLet’s now assume U to be finitely generated and let d denote its dimension. Here are a few useful facts:\n\nGiven an orthonormal basis (u_i)_{i = 1}^d of U, the coordinates of every vector x \\in U within (u_i) are (\\langle u_i, x\\rangle), i.e.: x = \\sum_{i = 1}^d \\langle u_i, x\\rangle u_i.\nThe norm of a vector x = \\sum x_i e_i expressed in an orthonormal basis (e_i) can easily be calculated from its coordinates: \\|x\\| = \\sum (x_i)^2.\nEvery orthogonal family (x_1, \\mathellipsis, x_k) of vectors of U is linearly independent, i.e. \\dim\\text{span}(x_1, \\mathellipsis, x_k) = k."
  },
  {
    "objectID": "posts/pca.html#orthogonal-projections",
    "href": "posts/pca.html#orthogonal-projections",
    "title": "An optimisation approach to Principal Component Analysis",
    "section": "1.3 Orthogonal projections",
    "text": "1.3 Orthogonal projections\nLet (x_i)_{i = 1}^k be an orthonormal family and let V denote the sub-vector space of U that it generates (i.e. V = \\text{span}(x_1, \\mathellipsis, x_k)). We define the orthogonal projection of U onto V, denoted \\pi_V, by: \\forall x \\in U: \\pi_V(x) = \\sum_{i = 1}^k \\langle x_i, x\\rangle x_i. It satisfies the following elementary properties:\n\n\\pi_V takes values in V;\nfor all v \\in V, \\pi_V(v) = v;\nfor all x, y \\in U, \\langle x, \\pi_V(y)\\rangle = \\langle \\pi_V(x), y\\rangle.\n\nA consequence of these properties is that U = V \\oplus V^\\bot, where V^\\bot is the set of vectors of U that are orthogonal to every vector of V. In other words, every vector x \\in U can be uniquely written as the sum of a vector in V (which is \\pi_V(x)) and a vector in V^\\bot.\nA more conceptually interesting property is that the orthogonal projection of some vector x \\in U onto V is the vector of V that is the closest to x (where close is defined in terms of the norm induced by the scalar product). Formally: \\|x - \\pi_V(x)\\| = \\min_{y \\in V} \\|x - y\\|.\nIn the introduction, we said that we needed to find an optimal sub-vector space and optimal points lying in this sub-vector space to approximate the original dataset. If we agree to measure closeness between datasets using a norm induced by a scalar product (for example, the Euclidean norm) then this result tells us that we actually only need to find an optimal sub-vector space, as we now have a formula for the optimal points."
  },
  {
    "objectID": "posts/linear_programming.html",
    "href": "posts/linear_programming.html",
    "title": "An introduction to Linear Programming",
    "section": "",
    "text": "This article defines linear problems and gives a rigorous introduction to the simplex algorithm that allows to solve it."
  },
  {
    "objectID": "posts/linear_programming.html#on-the-existence-of-a-solution",
    "href": "posts/linear_programming.html#on-the-existence-of-a-solution",
    "title": "An introduction to Linear Programming",
    "section": "1.1 On the existence of a solution",
    "text": "1.1 On the existence of a solution\nA (surprisingly non-trivial) result says that the set of linear combinations with non-negative coefficients of a finite family of vectors is closed. Formally, if A_1, \\mathellipsis, A_k are vectors lying in \\mathbb R^d then \\{\\sum_{i = 1}^k \\alpha_i A_i : (\\alpha_i) \\in [0, +\\infty)^k\\} is closed. We won’t prove this result here, but a nice, elementary proof can be found in this thread on math.stackexchange: Why are convex polyhedral cones closed?.\nA consequence of this result is that a linear problem has an optimal solution if and only if the objective function has a lower bound on the feasible set. To prove this, we consider the following matrix:\nC = \\begin{pmatrix}\nc_1 & \\dots & c_n \\\\\nA_{1, 1} & \\dots & A_{1, n} \\\\\n\\vdots & \\ddots & \\vdots \\\\\nA_{p, 1} & \\dots & A_{p, n}\n\\end{pmatrix}\nLet \\sigma be the infimum of the objective function over the feasible set and let (x_n) be a sequence of feasible solutions such that (\\langle c, x_n \\rangle) \\to \\sigma. Since (\\langle c, x_n \\rangle)_n and (Ax_n) (which is constant) converges towards some vector y^* \\in \\mathbb R^{p+1}. Notice that y^* = (\\sigma, b_1, \\mathellipsis, b_p). The closedness of the set of non-negative linear combinations of the columns of C (i.e. of the range of C under non-negative vectors) implies that there exists a non-negative vector x \\in \\mathbb R^n such that Cx = y^*. Such vector then satisfies \\langle c, x\\rangle = \\sigma, which shows that \\sigma is attained by a feasible solution."
  },
  {
    "objectID": "posts/linear_programming.html#vertices-and-optimal-solutions",
    "href": "posts/linear_programming.html#vertices-and-optimal-solutions",
    "title": "An introduction to Linear Programming",
    "section": "2.2 Vertices and optimal solutions",
    "text": "2.2 Vertices and optimal solutions\nA vertex of a convex set X is a point that does not lie on a segment between two other points of X, i.e. z is a vertex of X if z \\in X and for every x, y \\in X: z \\in [x, y] \\implies z \\in \\{x, y\\}. In other words, a vertex is a point z that cannot be written as \\lambda x + (1-\\lambda) y for some points x and y in X and \\lambda \\in (0, 1).\nRemember the linear problem we introduced earlier:\n\n\\begin{alignat*}1\n\\min\\quad& \\langle c, x\\rangle \\\\\n\\text{subject to}\\quad& x \\ge 0\\\\\n& Ax = b.\n\\end{alignat*}\n\n\n\n\nFeasible set of a linear problem defined by two inequalities and a non-negativity constraint.\n\n\nAn important result in the study of linear programming states that if a linear problem has an optimal solution, then there exists a vertex x^* of the feasible set such that \\langle c, x^*\\rangle is optimal.\n\n2.2.1 Proof that optimal solutions can be attained by a vertex\nTo prove the above result, we consider an optimal feasible solution x with minimum non-zero components and assume it is not a vertex. We can then write x = \\lambda y + (1-\\lambda)z for some feasible solutions y and z, \\lambda \\in (0, 1) and x \\notin \\{y, z\\}.\nTo find a contradiction, we’ll show that there exists a feasible solution on the straight line \\mathcal L passing through x and y which contains one more zero component than x.\nI will guide you through the proof and leave details in collapsible sections.\nWe first show that for every point u \\in \\mathcal L, if x_i = 0 then u_i = 0.\n\n\n\n\n\n\nProof\n\n\n\n\n\nSuppose, without loss of generality, that y_i \\le z_i. We can write 0 = \\lambda y_i + (1-\\lambda) z_i, which means that 0 \\in [y_i, z_i]. This implies that y_i \\le 0. Since y is a feasible solution, y_i \\ge 0, so y_i = 0. We can then write 0 = (1-\\lambda) z_i. Since x \\neq y, \\lambda \\notin \\{0, 1\\} so z_i = 0.\nIt follows that for every u \\in \\mathcal L with coordinate \\alpha (i.e. u = x + \\alpha (y-x)), u_i = 0 + \\alpha (0 - 0) = 0.\n\n\n\nThis implies that if x = 0, y = z = 0 which yields a contradiction. We now assume that x has at least one non-zero component.\nWe can also show that y and z are also optimal.\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe have \\langle c, x\\rangle = \\lambda \\langle c, y\\rangle + (1-\\lambda) \\langle c, z\\rangle with \\lambda \\notin \\{0, 1\\}. Assume, without loss of generality, that \\langle c, y\\rangle \\le \\langle c, z \\rangle. Then:\n\nif \\langle c, y\\rangle = \\langle c, z\\rangle then the proof is complete;\nif \\langle c, y\\rangle &lt; \\langle c, z\\rangle then \\langle c, x\\rangle \\in (\\langle c, y\\rangle, \\langle c, z\\rangle). But x is optimal so \\langle c, x\\rangle \\le \\langle c, y\\rangle: contradiction.\n\n\n\n\nIf we walk far enough away from x along \\mathcal L while remaining inside the feasible set, we’ll find a feasible solution z with one more zero-component than x – which contradicts an initial assumption completes the proof.\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe want to find \\alpha \\in \\mathbb R such that x + \\alpha(y - x) is non-negative and has at least one more zero component than x.\nLet i be an index such that x_i \\neq y_i (one must exist as we assumed that x \\neq y). Depending on the sign of x_i - y_i, a necessary and sufficient condition on \\alpha \\in \\mathbb R for the inequality x_i + \\alpha(y_i - x_i) \\ge 0 to hold is either \\alpha \\ge \\frac{x_i}{x_i - y_i} (if \\frac{x_i}{x_i - y_i} \\le 0) or \\alpha \\le \\frac{x_i}{x_i - y_i} (if \\frac{x_i}{x_i - y_i} \\ge 0). In any case, a sufficient condition for the inequality to hold is that |\\alpha| \\le \\left|\\frac{x_i}{x_i - y_i}\\right|.\nTo find \\alpha such that x + \\alpha (y-x) \\ge 0, we can then choose one that satisfies |\\alpha| \\le \\min_{i: \\left|x_i \\neq y_i} \\left\\{\\frac{x_i}{x_i - y_i}\\right|\\right\\}. Note that this defines a non-empty set of candidate values because x_i \\neq y_i implies that x_i \\neq 0.\nLet i \\in \\argmin_{i: \\left|x_i \\neq y_i} \\left\\{\\frac{x_i}{x_i - y_i}\\right|\\right\\} and let \\alpha = \\frac{x_i}{x_i - y_i}. The above constraint is indeed satisfied, x_i + \\alpha (y_i - x_i) = 0, which proves the claim."
  },
  {
    "objectID": "posts/linear_programming.html#convexity",
    "href": "posts/linear_programming.html#convexity",
    "title": "An introduction to Linear Programming",
    "section": "2.1 Convexity",
    "text": "2.1 Convexity\nIf x and y are vectors lying in the same space U then the segment between x and y is \\{\\lambda x + (1 - \\lambda)y: \\lambda \\in [0, 1]\\}.\n\n\n\n\n\n\nIntuition behind the definition of a segment\n\n\n\nTo convince yourself that this set does indeed describe something that looks like a segment, think of this procedure to construct a point lying in [x, y]:\n\nstart by going to x,\nto reach y from there, we need to walk y-x,\njust walk a fraction of it, i.e. \\lambda y - \\lambda x for some \\lambda \\in [0, 1],\nyou’re now at \\lambda y + (1-\\lambda)x!\n\n\n\nSimilarly, we can define the straight line that passes through x and y as \\{x + \\lambda(y-x): \\lambda \\in \\mathbb R\\}. Here’s a couple of simple results that will be useful later:\n\nif \\phi is a linear map, \\phi(u) = \\phi(v) implies that \\phi(u) = \\phi(w) for every w on the straight line that passes through u and v,\nif x and y are distinct real numbers and z = \\lambda x + (1 - \\lambda) y for some \\lambda \\in [0, 1], then z \\in \\{x, y\\} implies \\lambda \\in \\{0, 1\\}.\n\nA set X is said to be convex if for every x, y \\in X, [x, y] \\sube X. It is easy to show that the feasible set of a linear problem is convex."
  },
  {
    "objectID": "posts/linear_programming.html#expand-to-learn-about-collapse",
    "href": "posts/linear_programming.html#expand-to-learn-about-collapse",
    "title": "An introduction to Linear Programming",
    "section": "2.3 Expand To Learn About Collapse",
    "text": "2.3 Expand To Learn About Collapse\nThis is an example of a ‘folded’ caution callout that can be expanded by the user. You can use collapse=\"true\" to collapse it by default or collapse=\"false\" to make a collapsible callout that is expanded by default."
  },
  {
    "objectID": "posts/linear_programming.html#basic-feasible-solutions-and-an-algebraic-way-of",
    "href": "posts/linear_programming.html#basic-feasible-solutions-and-an-algebraic-way-of",
    "title": "An introduction to Linear Programming",
    "section": "2.3 Basic feasible solutions and an algebraic way of",
    "text": "2.3 Basic feasible solutions and an algebraic way of"
  },
  {
    "objectID": "posts/linear_programming.html#basic-feasible-solutions",
    "href": "posts/linear_programming.html#basic-feasible-solutions",
    "title": "An introduction to Linear Programming",
    "section": "2.3 Basic feasible solutions",
    "text": "2.3 Basic feasible solutions\nFeasible sets have finitely many vertices, which motivates the low-level idea of the simplex algorithm: we’ll enumerate all vertices until we find an optimal one. In this section, we’ll introduce basic feasible solutions which allow to algebraically characterise vertices of the feasible set and constitute a building block of the simplex algorithm.\nLet x be a feasible solution and let I = \\{i \\in \\{1, \\mathellipsis, n\\} : x_i \\neq 0\\} be its support. x is a basic feasible solution if \\{A_{\\bullet, i} : i \\in I\\} is linearly independent, where A_{\\bullet, i} is the i-th column of A.\nWe saw that if a basic feasible solution x lies on a segment [y, z] of feasible solutions with x \\notin \\{y, z\\} then y and z are both basic feasible solutions and that they both have the same support as x. But if two basic feasible solutions u and v have the same support I, then Au = \\sum_{i \\in I} A_{\\bullet, i} u_i = Av = \\sum_{i \\in I} A_{\\bullet, i} v_i – since (A_{\\bullet, i})_{i \\in I} is linearly independent, we must have u = v. This proves that every basic feasible solution is a vertex of the feasible set!\nConversely, we can show that if a feasible solution x is not basic then is not a vertex.\n\n\n\n\n\n\nProof\n\n\n\n\n\nWe’ll show that there exists a vector \\alpha \\neq 0 such that x - \\alpha and x + \\alpha are both feasible. Since x = \\frac{1}{2}(x-\\alpha) + \\frac{1}{2}(x + \\alpha), this will suffice to prove that x is not a vertex.\nFor \\alpha to satisfy A(x-\\alpha) = A(x+\\alpha) = b, we need to have A\\alpha = 0. This is possible because (A_{\\bullet, i})_{i \\in I} is not linearly independent: we can find (\\alpha_i)_{i \\in I} \\neq 0 such that \\sum \\alpha_i A_{\\bullet, i} = 0, which we can complete into a vector lying in \\mathbb R^p by adding zero components.\nIf A\\alpha = 0 then A(\\varepsilon \\alpha) = 0 for every \\varepsilon &gt; 0, so we can scale down \\alpha by applying a factor \\varepsilon small enough for both x-\\alpha and x+\\alpha to be non-negative, therefore feasible – this completes the proof.\nThis strategy could have been used to prove the equivalence directly: if x is not a vertex and y and z are feasible solutions such that x \\in [y, z] \\setminus \\{y, z\\} then we can always find \\alpha \\neq 0 such that x = \\frac{1}{2}(x-\\alpha) + \\frac{1}{2}(x+\\alpha), x-\\alpha \\in [y, z] and x+\\alpha \\in [y, z] – we’d then have A(x-\\alpha) = b (x-\\alpha is on the segment between two feasible solutions, so it has to be feasible itself) so A\\alpha = 0, and \\alpha_i = 0 when i \\notin I ; therefore (A_{\\bullet, i})_{i \\in I} cannot be linearly independent and x is not a basic feasible solution."
  },
  {
    "objectID": "posts/linear_programming.html#technical-priors",
    "href": "posts/linear_programming.html#technical-priors",
    "title": "An introduction to Linear Programming",
    "section": "3.1 Technical priors",
    "text": "3.1 Technical priors\nWe first need to prove a few technical results that allow to efficiently make use of the general results we saw earlier. We start by fixing a basis of \\mathbb R^p consisting of columns of A and rearrange the columns of A to write it as the concatenation of two matrices B \\in \\mathbb R^{p, p} and N \\in \\mathbb R^{p, n-p}: A = (B|N), where the columns of B are linearly independent. If we rearrange the variables and the vector of the objective function with the same partition (we’ll write x = (x_B | x_N) and c = (c_B | c_N)), we have \\langle c, x\\rangle = \\langle c_B, x_B\\rangle + \\langle c_N, x_N\\rangle and Ax = Bx_B + Nx_N.\nSince the columns of B are linearly independent, B is invertible. If x \\in \\mathbb R^n is a basic feasible solution such that x_N = 0 (we’ll say that x is adapted to partition (B, N)), we have: Ax = Bx_B + Nx_N = Bx_B = b, therefore x_B = B^{-1}b. In other words, we’ve shown that choosing a basis of \\mathbb R^p consisting of columns of A allows to deterministically define a vertex of the feasible set – and to calculate it easily (computers know how to invert matrices).\n\n3.1.1 Tell whether a vertex is optimal\nLet x be the basic feasible solution adapted to (B, N) and y = (y_B | y_N) be an arbitrary feasible solution. Let \\Delta = \\langle c, x\\rangle - \\langle c, y\\rangle. Then \\Delta = \\langle c_B, B^{-1}b\\rangle - \\langle c_B, y_B\\rangle - \\langle c_N, y_N\\rangle.\nSince By_B + Ny_N = b, y_B = B^{-1}(b - Ny_N) so we can write: \\begin{alignat*}{1}\n\\Delta &= \\langle c_B, B^{-1}b - \\langle c_B, B^{-1}(b-Ny_N)\\rangle - \\langle c_N, y_N \\rangle\\\\\n&= \\langle c_B, B^{-1}Ny_N \\rangle - \\langle c_N, y_N \\rangle\\\\\n&= \\langle (B^{-1}N)^Tc_B, y_N \\rangle - \\langle c_N y_N \\rangle\\\\\n&= \\langle (B^{-1}N)^Tc_B - c_N, y_N\\rangle.\n\\end{alignat*}\nThis shows that a sufficient condition for x to be optimal is (B^{-1}N)^T c_B - c_N \\le 0. It turns out that if we further assume that x_B &gt; 0, this condition is also necessary."
  }
]