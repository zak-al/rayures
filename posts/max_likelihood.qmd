---
title: An introduction to maximum likelihood estimation and applications to machine learning
categories:
    - Statistics
    - Machine learning
---

The _maximum likelihood_ paradigm describes a rule to find a distribution that best fits a set of examples among a parametrised set of distributions. This article defines maximum likelihood estimators and gives two examples of learning models motivated by this paradigm.

# Theoretical foundations

## Motivation

Suppose you have a parametrised set of probability distributions, characterised by density functions with respect to the counting or Lebesgue measure:
$$\{f_\vartheta : X \to \R_+\}_{\vartheta \in \Theta}.$$

Since these measures are closed under translation, we can view the value of their density $f_\vartheta(x)$ at some point $x \in \R$ as an _immediate probability_ (note that this name can be misleading as it is _not_ a probability in the case of Lebesgue-continuous distributions).
If we work with the counting measure, this is clear since the value of the density at some point is the actual probability of the point. When working with the Lebesgue measure, this get slightly more subtle. If the density function is continuous, we can formally justify this intuition by noticing that $f_\vartheta(x)$ is the limit of the average probability around $x$: $\lim_{\delta \to 0} \frac{1}{\delta} \int_{\left[x - \frac\delta2, x + \frac\delta2\right]} f_\vartheta\ \text d\mu$, which is the fundamental theorem of analysis.
This result can be generalised using the Lebesgue differentiation theorem ([Wikipedia](https://en.wikipedia.org/wiki/Lebesgue_differentiation_theorem))

## General definition

From this intuition, we can define the _likelihood_ of a sequence of independent and identically-distributed observations $\mathbf{x} = \{x_1, \mathellipsis, x_n\}$ as $L(\vartheta) = f_\vartheta^n(\mathbf{x})$, where $f_\vartheta^n$ is the joint probability distribution over sequences of $n$ independent $f_\vartheta$-distributed examples. We then have:
$$L(\vartheta) = \prod_{j = 1}^n f_\vartheta(x_j).$$

From a computational perspective, we often prefer to deal with the $\log$-likelihood of a sequence of examples rather than their likelihood:

$$\log L(\vartheta) = \sum_{j = 1}^n \log\left(f_\vartheta(x_j)\right).$$

We then define the maximum-likelihood estimator of $\vartheta$ as:

$$\hat\vartheta_{\text{ml}} = \argmax_{\vartheta \in \Theta} L(\vartheta),$$

where the existence and uniqueness of the argmax is assumed.

Note that since $\log$ is strictly increasing, $\argmax_\vartheta L(\vartheta) = \argmax_\vartheta (\log(L(\vartheta)))$.

## Approximating distribution of input/output pairs with conditional maximumum likelihood

Similarly, we can use the maximum likelihood paradigm to find a distribution that best describes a set of observations of the form $\{(x_1, y_1), \mathellipsis, (x_n, y_n)\}$, where $y_j$ is viewed as an output associated with input $x_j.$ For every $x_j$, we consider the density functions $f_\vartheta(\cdot | x_j)$ across all $\vartheta \in \Theta$. We then define the likelihood of the set of observations for parameter $\vartheta$ as the density function of the random vector sequence $(Y_1, \mathellipsis, Y_n)$, where $Y_j \sim f_\theta(\cdot | x_j)$, evaluated at $(y_1, \mathellipsis, y_n)$ i.e.
$$L(\vartheta) = \prod_{j = 1}^n f_\vartheta(y_j | x_j).$$

# Application: constructing loss functions

We can apply this theory to machine learning by constructing loss functions that decrease as the likelihood of the parameters increases.
To do this, we'll view machine learning models as approximating distributions rather than making single predictions.
In other words, instead of a model predicting one output $y$ given some input $x$, it will find parameters that define a distribution over the set of possible outputs such that the prediction is the output where the density is the highest.

## Application to regression models

Regression models seek to predict a real number $y$ based on an observation $x$.
To translate this definition into our probabilistic framework, we'll assume that given an observation $x$, possible predictions are uniformly distributed around some mean $\mu$ with some standard deviation $\sigma^2$.
For simplicity, we'll assume that the standard deviation is fixed across all inputs, and that it equals $1$. Our goal is therefore to approximate $\mu$.
The most probable outcome when making a normally-distributed draw over $\R$ being the mean of the distribution, $\mu$ corresponds to what the model would predict in a single-prediction framework.

Suppose you have a dataset $D = \{(x^{(i)}, y^{(i)}) : 1 \le i \le N\}$ where $x^{(i)} \in U$ are inputs, lying in some space $U$, and $y^{(i)} \in \R$ are predicted values.
We describe our model by a function $f_W : U \to \R$ that depends on a parameter $W$.
The likelihood of the dataset given some parameter $W$ is:

$$L(\theta) = \prod_{i = 1}^N \alpha \exp\left(-\frac12(y^{(i)}-f_{W}(x^{(i)}))^2\right),$$

where $\alpha$ is a positive constant.

The $\log$-likelihood is then:
$$-\frac12 \sum_{i = 1}^N (f_W(x^{(i)}) - y^{(i)})^2 + \text{ stuff}.$$

In machine learning, we often consider _cost functions_ that we seek to minimise, so we actually want to minimise:
$$\sum_{i = 1}^N (f_W(x^{(i)}) - y^{(i)})^2.$$

Grouping terms together, we show that if the output contains multiple units (in other words, if we want to find a normally-distributed vector), the cost function becomes:
$$\sum_{i = 1}^N \|f_W(x^{(i)}) - y^{(i)}\|^2,$$
where $\|\cdot\|$ is the $L^2$ norm.

## Application to classification

We now suppose that our dataset $D$ contains points $(x^{(i)}, y^{(i)}) \in U \times \{0, 1\}$ for $1 \le i \le N$, where $U$ is an arbitrary input space.
The model is described by a function $f_W: U \to (0, 1)$ that assigns an input to the probability of the associated output being $1$, thus defining a Bernoulli distribution.
The logarithm of the probability of $k \in \{0, 1\}$ for parameter $W$ given input $x^{(i)}$ is therefore $k \log\left(f_W(x^{(i)})\right) + (1-k)\log\left(1 - f_W(x^{(i)})\right)$.
The $\log$-likelihood is:
$$\sum_{i = 1}^N y^{(i)} \log\left(f_W(x^{(i)})\right) + (1-y^{(i)})\log\left(1 - f_W(x^{(i)})\right).$$

and the cost function we want to minimise is:
$$-\sum_{i = 1}^N y^{(i)} \log\left(f_W(x^{(i)})\right) + (1-y^{(i)})\log\left(1 - f_W(x^{(i)})\right).$$

We can generalise this formula to any classification problem.
Consider a set of $k$ classes (without loss of generality, we'll assume classes are $\{1, \mathellipsis, k\}$).
Instead of computing a single real number, $f_W$ will now compute a vector of $k$ positive numbers adding up to $1$.
For all $i \in \{1, \mathellipsis, k\}$, the $i$-th component of the output of $f_W$ is an estimation of the probability of class $i$.
This naturally defines a probability distribution over the set of all classes.

Given an input $x^{(i)}$, the $\log$-likelihood of output $y^{(i)}$ for parameter $W$ is then:
$$\log f_W(x^{(i)})_y^{(i)},$$

and the cost of the dataset is:
$$-\sum_{i = 1}^N \log f_W(x^{(i)})_{y^{(i)}}.$$
